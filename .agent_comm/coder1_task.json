{
  "agent_id": "coder1",
  "task_id": "task_6",
  "files": [
    {
      "name": "training.py",
      "purpose": "Agent training pipeline",
      "priority": "high"
    },
    {
      "name": "policy.py",
      "purpose": "Policy network implementation",
      "priority": "high"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.CL_2508.10824v1_Memory_Augmented_Transformers_A_Systematic_Review",
    "project_type": "agent",
    "description": "Enhanced AI project based on cs.CL_2508.10824v1_Memory-Augmented-Transformers-A-Systematic-Review with content analysis. Detected project type: agent (confidence score: 11 matches).",
    "key_algorithms": [
      "Language",
      "Coupling",
      "Evolutionary",
      "Workspace",
      "Conceptual",
      "Original",
      "Surprise",
      "Reasoning",
      "Transfer",
      "Autoassociative"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "\n--- chunk_1.txt ---\nPDF: cs.CL_2508.10824v1_Memory-Augmented-Transformers-A-Systematic-Review.pdf\nChunk: 1/2\n==================================================\n\n--- Page 1 ---\nMemory-Augmented Transformers: A Systematic Review\nfrom Neuroscience Principles to Technical Solutions\nParsa Omidi parsa.omidi@huawei.com\nHuawei Technologies\nXingshuai Huang xingshuai.huang@h-partners.com\nHuawei Technologies\nAxel Laborieux axel.laborieux@huawei.com\nHuawei Technologies\nBahareh Nikpour bahar.nikpour@h-partners.com\nHuawei Technologies\nTianyu Shi tianyu.shi@h-partners.com\nHuawei Technologies\nArmaghan Eshaghi armaghan.eshaghi@huawei.com\nHuawei Technologies\nAbstract\nMemory is fundamental to intelligence, enabling learning, reasoning, and adaptability across\nbiological and artificial systems. While Transformer architectures excel at sequence mod-\neling, they face critical limitations in long-range context retention, continual learning, and\nknowledge integration. This review presents a unified framework bridging neuroscience prin-\nciples\u2014dynamic multi-timescale memory, selective attention, and consolidation\u2014with engi-\nneeringadvancesinMemory-AugmentedTransformers. Weorganizerecentprogressthrough\nthree taxonomic dimensions: functional objectives (context extension, reasoning, knowledge\nintegration, adaptation), memory representations (parameter-encoded, state-based, explicit,\nhybrid), and integration mechanisms (attention fusion, gated control, associative retrieval).\nOur analysis of core memory operations\u2014reading, writing, forgetting, and capacity man-\nagement\u2014reveals a shift from static caches toward adaptive, test-time learning systems.\nWe identify persistent challenges in scalability and interference, alongside emerging solu-\ntions including hierarchical buffering and surprise-gated updates. This synthesis provides a\nroadmap toward cognitively-inspired, lifelong-learning Transformer architectures.\n1 Introduction\nMemory is fundamental to both biological and artificial intelligence (AI), serving as the foundation for cog-\nnition, reasoning, and adaptive learning (Camina & G\u00fcell, 2017). In humans, memory enables the retention,\nretrieval, and manipulation of information across multiple time scales, supporting complex behaviors such as\ndecision-making and problem-solving Wang et al. (2025a). This dynamic process integrates sensory inputs,\ntransient processing, and long-term storage, forming a sophisticated cognitive architecture.\nIn AI, memory has become increasingly central as models evolve from static pattern recognition to more\nflexible, human-like cognition. Transformer architectures (Vaswani, 2017) have significantly advanced nat-\nural language processing, vision, and multimodal learning, yet their memory mechanisms remain restricted\ncompared to the flexibility and efficiency of biological systems.\n1arXiv:2508.10824v1  [cs.LG]  14 Aug 2025\n\n--- Page 2 ---\nThe primary limitation arises from self-attention\u2019s quadratic complexity, constraining context window sizes.\nTo stay within hardware limits, techniques like token pruning, sparse attention, and KV caching extend\ncontext but at a fidelity cost: sparse or approximate attention fractures long-range dependencies, and KV\ncaches must evict or compress older entries, discarding vital information and harming coherence (Wang et al.,\n2024a). Another issue is the static nature of knowledge representation in standard Transformers. Once\ntrained, their parameters are fixed, lacking mechanisms for continual learning or dynamic updates. This\nrigidity hinders adaptation to new information or user-specific contexts and risks catastrophic forgetting\nwhen fine-tuned, unlike the flexible updating seen in biological memory. Transformers also lag far behind\nbiological systems in energy efficiency. The brain uses sparse, distributed, content-addressable memory with\nlocalizedsynapticdynamics,operatingonmilliwattsofpower(Princeetal.,2016;Gilbert&Brushfield,2009).\nIn contrast, Transformers require intensive computation: full-context inference scales quadratically with\nsequence length, while autoregressive decoding must process ever-growing KV caches with linear complexity\nper token. This computational burden results in orders-of-magnitude higher energy consumption.\nTo bridge these gaps, memory-augmented Transformers integrate neuroscience-inspired dynamic mem-\nory mechanisms. Human memory\u2019s efficiency and adaptability increasingly guide Transformer design,\nparticularly its integration across timescales: sensory memory (brief stimulus retention), prefrontal\ncortex-maintained working memory (short-term processing), and long-term memory (lifelong learning via\nneocortical-hippocampal networks). This architecture balances immediate processing with stable knowledge\nretention, while memory allocation is further regulated by salience and context-focusing attention only on\nrelevant inputs, as described by the global workspace theory (Dehaene et al., 2011; Baars et al., 2021).\nThese neuroscience-derived principles increasingly shape memory-augmented Transformer architectures. Re-\ncent models incorporate multi-timescale memory, dynamic resource allocation, and plasticity-stability trade-\noffs, drawing explicit inspiration from hippocampal indexing, neuromodulatory gating, and hierarchical\norganization.\nRecent review efforts have explored memory structures in AI models from various angles. For example, Ma\net al. (2023) surveys memory augmentation techniques specifically in graph neural networks (GNNs), while\nDu et al. (2025) offers a broader perspective, covering diverse memory mechanisms across AI models, includ-\ning long-term memory, long-context memory, parametric memory modification, and multi-source memory.\nHe et al. (2024b) approaches the topic from a human-inspired perspective, focusing on long-term memory\nin AI models. Other surveys narrow their scope further: Shan et al. (2025) and Wu et al. (2025) focus on\nmemory mechanisms in large language models (LLMs), while Zhang et al. (2024) specifically investigates\nmemory in LLM-based agents. Similarly, Liu et al. (2025) includes a dedicated chapter on memory usage in\nfoundation agents.\nHowever, existing reviews are limited in two key ways. First, most rely on a single taxonomy to categorize\nmemory-augmented methods, failing to provide a multidimensional or interdisciplinary understanding of the\nfield. Second, many focus narrowly on specific model types or memory paradigms, such as long-term memory,\nLLMs, or agents, without addressing the broader landscape of memory integration across Transformer-based\nmodels.\nIn contrast, our review provides a comprehensive and interdisciplinary examination of memory augmentation\ntechniques across Transformer models of various sizes, types, and applications. Our objectives are as follows:\n\u2022Establish comprehensive taxonomies linking neuroscience principles to memory mechanisms in\nTransformers from three different aspects.\n\u2022Analyze core memory operations , including reading, writing, forgetting, and self-management.\n\u2022Identify current challenges in memory-augmented Transformer design and highlight emerging\nparadigms and future directions inspired by biological memory.\nBy integrating insights from neuroscience and AI, this review aims to provide a conceptual framework\nand practical guidance for developing more efficient, adaptive, and cognitively inspired memory-augmented\nTransformers.\n2\n\n--- Page 3 ---\nSensory MemoryWorking MemoryLong-Term MemoryConsolidationAttentionRehearsal, elaboration, or chunkingRetrievalSensory ReceptionEnvironment\nFeature MapsAttention ContextParametric/Non-parametric MemoryUpdateAttention MechanismEncodingRetrievalEmbeddingEnvironmentHUMAN BRAIN\nMEMORY-AUGMENTED TRANSFORMER\nFigure 1: Parallels between the memory systems in the human brain and memory-augmented Transformers.\nHuman memory consists of three interacting subsystems: sensory memory, working memory, and long-term\nmemory. Memory-augmented Transformers mirror this architecture by leveraging embeddings, attention\nmechanisms, and advanced encoding and retrieval techniques to construct feature maps (analogous to short-\nterm memory), attention contexts (analogous to working memory), and parametric or non-parametric mem-\nory (analogous to long-term memory).\nIn the following sections, we begin by introducing memory architectures in biological cognitive systems,\nincluding the structure of human memory (Section 2.1), interactions between different memory systems\n(Section 2.2), and underlying computational principles (Section 2.3). Section 3 presents our proposed tax-\nonomies from three perspectives: functional objectives (Section 3.1), memory types (Section 3.2), and inte-\ngration techniques (Section 3.3). We then examine the mechanisms of memory operations adopted in the\nreviewed methods (Section 4), followed by a discussion of key challenges and future directions (Section 5).\n2 Memory Architectures in Biological Cognitive Systems\nHuman memory operates as an interconnected, multi-layer network that stores, retrieves, and adapts infor-\nmation across several time-scales. Because these operations are hierarchical and widely distributed, stored\nknowledge is continuously reorganised, supporting rapid perception, flexible reasoning, and lifelong learn-\ning. This section reviews the biological architecture of memory and extracts principles that can inform\ncognitively-inspired AI models.\n2.1 Architecture of Human Memory\nRather than a single store, human memory comprises three interacting subsystems, i.e., sensory, working,\nand long-term memory, as shown in the upper part of Figure 1. Each of them is optimised for a distinct\ncombination of capacity, persistence, and processing depth (Cowan, 2008). Together they enable perception,\ndecision-making, and learning across milliseconds to decades.\nSensory Memory: the Initial Buffer. Sensory memory provides a high-bandwidth, ultra-short buffer for\nraw perceptual input: visual traces (iconic) persist for \u2248250 ms and auditory traces (echoic) for up to 2\u20133s\n(Reznik et al., 2023). In that brief window the brain analyses many stimuli in parallel; only items flagged by\nattention transition to working memory, while the rest decay rapidly, preventing overload. Neurally, these\ntransienttracesarisefromsustainedactivityinprimarysensorycorticesandthalamo-corticalloops,organised\ninto modality-specific registers that filter noise and normalise signals before further processing (Camina &\nG\u00fcell, 2017). Transformers mimic part of this stage via token embeddings and positional encodings, which\nstabilise raw inputs for downstream layers. Yet, unlike biological circuits that adapt gain and leverage\n3\n\n--- Page 4 ---\noscillations for temporal binding, current AI pipelines remain static, making robust perception under noise\nand context-dependent retention an open challenge for memory-augmented models.\nWorking Memory: the Cognitive Workspace. Working memory provides a transient, capacity-limited\nworkspace that actively maintains and manipulates information required for reasoning, problem-solving,\nand goal-directed behaviour (Miller, 1956; Baddeley, 2003). Empirical estimates place its span at roughly\nfour to seven \u201cchunks,\u201d a limit mitigated by chunking strategies and sustained by oscillatory activity in the\nprefrontal\u2013parietal network.\nPersistent firing, which is often organised through theta\u2013gamma coupling, keeps multiple representations\nsimultaneously accessible, while dopaminergic signals from the ventral tegmental area gate updates, suppress\ndistractions, and prioritise task-relevant items (Roux & Uhlhaas, 2014). Cross-modal binding is supported\nby beta-band synchrony that links prefrontal cortex with hippocampal and sensory regions, enabling flexible\nrecombination of auditory, visual, and spatial cues during complex tasks (Quak et al., 2015).\nFunctionally, the prefrontal cortex operates as a central executive, allocating attention, switching tasks, and\ncoordinating specialised buffers (e.g., phonological loop, visuospatial sketchpad) Russin et al. (2020). This\ndistributed control balances stability with rapid updating, allowing the system to adapt to changing demands\nwhile avoiding interference.\nTransformer self-attention partially echoes these operations by selectively weighting tokens within a fixed\ncontext window. Yet current models lack biologically inspired features such as neuromodulatory gating,\noscillatory binding, and energy-efficient recall; external memories and recurrent variants narrow the gap but\nhave yet to match the flexibility and robustness of human working memory.\nLong-Term Memory: the Knowledge Repository. Long-term memory (LTM) is the brain\u2019s durable\nstorehouse, capable of retaining knowledge and experience for years or even a lifetime. Its defining strength\nis persistence: after consolidation, a trace can remain accessible indefinitely, provided it is periodically\nreactivated. Information is organised hierarchically into interconnected schemas that accelerate retrieval\nand support broad generalisation, yet the system stays plastic because each act of recall can render a trace\ntemporarily labile and open to updating before it is re-stored during reconsolidation (Luo et al., 2022; Lee\net al., 2017).\nTwo complementary consolidation processes underpin this durability. Synaptic consolidation , completed\nwithin hours, strengthens hippocampal circuits through activity-dependent events such as calcium spikes\nand sharp-wave ripples (Mujawar et al., 2021). Systems consolidation unfolds over days to years, as co-\nordinated oscillations during sleep transfer memory indices from the hippocampus to distributed neocortical\nnetworks, creating resilient, cortex-based representations that can survive hippocampal damage (Luo et al.,\n2022).\nLTM comprises episodic and semantic subsystems. Episodic memory records personally experienced\nevents tied to a specific time and place and relies on hippocampal pattern completion for cue-based recall.\nSemantic memory stores abstract facts and concepts in widely distributed cortical networks, allowing\nindividuals to answer questions like a capital city\u2019s name without re-living the original learning episode\n(Kumar, 2021). The interplay of these subsystems enables both vivid recollection and flexible inference.\nAdult neurogenesis in the dentate gyrus adds further adaptability, inserting new neurons that improve\npattern separation and support the incorporation of novel information without erasing older traces (Anacker\n& Hen, 2017). This continual renewal helps the brain distinguish similar experiences and maintain cognitive\nflexibility across the lifespan.\nCurrent AI systems approximate LTM with a mix of parameter-encoded knowledge and external memories.\nParameter storage offers instant access but is costly to update, whereas external key\u2013value banks such as\nMemformer\u2019s fixed-size slots (Wu et al., 2020) or EMAT\u2019s compressed QA memories (Wu et al., 2022b) allow\non-the-fly writes and reads at inference time. Retrieval-Augmented Generation (RAG) (Lewis et al., 2020)\nextends this idea by fetching fresh documents from external indices before every response, giving models\na dynamic knowledge base. Despite these advances, artificial LTM still suffers from limited consolidation\nand vulnerability to catastrophic forgetting when new data overwrites old weights (Ranjith & Baskaran,\n4\n\n--- Page 5 ---\n2024). Closing this gap will require biologically inspired mechanisms, e.g., dynamic consolidation, adaptive\nforgetting, and hierarchical memory layouts, that mirror the robustness and context sensitivity of human\nlong-term memory.\n2.2 Interactions Between Memory Systems\nHuman memory functions as a dynamic network, where sensory, working, and long-term stores communicate\ncontinuously to maximize learning and behaviour. Instead of isolated modules, these systems exchange ac-\ntivity through converging cortical\u2013subcortical loops that adapt to context, attention, and emotional salience.\nEncoding, Consolidation, and Retrieval. Encoding begins when sensory traces reach the prefrontal\ncortex, which filters and amplifies task-relevant inputs before they flow into working and long-term stores.\nSubsequent consolidation\u2014particularly during slow-wave sleep\u2014relies on hippocampal replay that drives\nneocortical reorganisation, stabilising both episodic and semantic traces (Klinzing et al., 2019). Retrieval\ncompletes this cycle: a partial cue reactivates hippocampal indices, triggering pattern-completion processes\nthat reconstruct the distributed cortical representation and return it to working memory for use or further\nupdating Teyler & Rudy (2007).\nTop-Down and Bottom-Up Modulation. During retrieval, top-down signals from prefrontal regions bias\nprocessingtowardcurrentgoals, suppressingirrelevantinformation, whilebottom-upinputsfromsensoryand\nlimbic areas flag novelty or emotional significance. Neuromodulators such as dopamine and acetylcholine\nstrengthen synapses that encode behaviourally important events, fine-tuning what is stored or updated\n(Gazzaley & Nobre, 2012).\nEmotional and Multimodal Integration. Emotionally charged or multisensory experiences recruit co-\nordinated activity in the amygdala, hippocampus, and prefrontal cortex, yielding more persistent memories\n(Dolcos et al., 2004). The thalamus binds inputs from different senses, and hippocampal pattern completion\nlinks them into context-rich episodes that can be triggered later by a single cue.\nCompetitiveandCo-operativeDynamics. Memorysystemsshiftbetweencompetitionandcooperation.\nUnder stress or heavy cognitive load, control can pass from flexible hippocampal networks to faster, habit-\nbased striatal circuits, ensuring rapid action (Schwabe & Wolf, 2013). In calmer conditions, episodic and\nsemantic stores collaborate: detailed recollections supply context while abstract schemas guide generalisation\nand planning (Moscovitch et al., 2016).\nDefault Mode Network and Predictive Processing. The Default Mode Network supports offline\nconsolidation, autobiographical recall, and mental simulation (Higgins et al., 2021). By replaying prior ex-\nperiences, it updates internal models, enabling predictive processing that helps the brain (and, by extension,\nAI agents) anticipate future events and adapt behaviour accordingly (Liu et al., 2021).\nUnderstanding this balance of stability and plasticity offers a template for AI: memory architectures that\ncoordinate fast buffers with slower, more permanent stores, employ selective gating, and integrate cross-\nmodal information can move beyond static storage toward lifelong, context-aware learning.\n2.3 Computational Principles from Biological Memory\nThe architectural and functional properties of biological memory systems reveal fundamental computational\nprinciples that guide memory-augmented transformer design. These principles address universal computa-\ntional challenges: managing limited resources, balancing stability with plasticity, and coordinating informa-\ntion flow across multiple timescales. Abstracting these neurobiological solutions into engineering heuristics\nyields a practical design playbook, now guiding the development of the most effective memory-augmented\nTransformer architectures.\nHierarchical Resource Allocation. Biological memory demonstrates that computational efficiency\nemerges from hierarchical organization rather than uniform processing (Hasson et al., 2015). Sensory\nmemory\u2019s high-bandwidth, ultra-short retention enables parallel pre-processing, working memory\u2019s capacity-\nlimited workspace allows flexible manipulation, and long-term memory\u2019s distributed storage supports both\nrapid recall and gradual consolidation. Multimodal evidence suggests that these hierarchical dynamics\n5\n\n--- Page 6 ---\nemerge as a global organizing principle of mammalian brains, with cortical timescale gradients topographi-\ncally mirrored in striatum, thalamus, and cerebellum (Raut et al., 2020). This hierarchical structure suggests\nthat artificial systems benefit from multi-tier memory architectures that match storage characteristics to com-\nputational demands .\nAttention-Memory Bidirectional Coupling. The interaction between attention and memory reveals\na crucial computational principle: memory systems both shape and are shaped by attentional mechanisms\n(Chun & Turk-Browne, 2007). Extensive evidence demonstrates that attention and memory cannot operate\nwithout each other: memory has limited capacity and attention determines what will be encoded, while\nmemory from past experience guides what should be attended. Brain areas important for memory, such as\nthe hippocampus and medial temporal lobe structures, are recruited in attention tasks, and memory directly\naffects frontal-parietal networks involved in spatial orienting. This bidirectional coupling enables adaptive\nresource allocation and context-sensitive processing through attention-dependent coupling between forebrain\nand brainstem neuromodulatory systems (Cicero et al., 2025). These principles suggest AI memory systems\nshould incorporate feedback loops between retrieval mechanisms and encoding processes .\nNeuromodulatory Gating and Significance Filtering. Biological memory formation relies on the\ninterplay between Hebbian plasticity and neuromodulatory systems, making memory encoding inherently\nstate-dependent and gated by the behavioral significance of information (Bazzari & Parri, 2019). Neuro-\nmodulators such as dopamine and acetylcholine have distinct and complementary roles: dopamine regulates\nthe induction of synaptic plasticity by modulating glutamatergic signaling, while acetylcholine orchestrates\nneuronal activity at both synaptic and network-wide levels. These systems establish computational princi-\nples of selective attention and adaptive thresholding, which allow the brain to prioritize salient information\nfor encoding. Notably, selective neuromodulatory gating reflects a fundamental asymmetry in biological\ncognition: less than 5% of brain activity is devoted to conscious processes, while over 95% operates un-\nconsciously, thus maximizing efficiency and resource allocation Raichle et al. (2001) Nail (2021). The strict\nlimitations of working memory\u2014estimated at 4\u20137 meaningful chunks\u2014essentially define the conscious mind\u2019s\ncomputational budget. This insight suggests that artificial memory systems should carefully reserve costly,\nconscious-like processing for high-priority tasks such as novelty detection or conflict resolution, while routine\nmemory operations are best delegated to automatic, parallel processing pathways analogous to the brain\u2019s\nunconscious majority .\nReplay-Based Consolidation and Interference Management. The brain\u2019s solution to the stability-\nplasticity dilemma through dual-phase consolidation, i.e., rapid hippocampal encoding followed by gradual\nneocortical integration, reveals essential computational principles for managing memory interference (Squire\net al., 2015). Neural replay during sleep drives consolidation by reactivating patterns of network activity that\noccurred during previous experience, leading to potentiation of relevant synaptic connections in the cortex.\nThis process enables rapid learning without catastrophic forgetting through replay-based consolidation and\nsystems-level reorganization, where hippocampal replay propagates to cortex with reprocessing to extract\nstatistical overlap from different encoding episodes. The stability-plasticity dilemma reflects a fundamental\nchallenge in learning systems: retaining stored memory while learning new information (Mermillod et al.,\n2013).The implication for AI is that effective memory systems require complementary fast and slow learning\nmechanisms with explicit consolidation phases .\nContent-Addressable Associative Retrieval. Biological memory systems excel at content-addressable\nretrieval through associative networks that enable pattern completion from partial cues (Rolls, 2013). The\nhippocampal CA3 subfield functions as an autoassociative network that stores experiences as memories,\nwith abundant recurrent connections exhibiting spike-timing-dependent plasticity that allows pattern com-\npletion and recovery of stored patterns from noisy cues (Kang & Toyoizumi, 2024). Empirical evidence from\ndirect hippocampal recordings reveals pattern completion mechanisms where reinstatement of encoding pat-\nterns occurs during successful recollection, linked to gamma power fluctuations that coordinate selection\nof target-relevant neurons (Staresina et al., 2016). The CA3 region exemplifies this through its ability to\nretrieve complete episodic memories from fragmentary input, while semantic memory supports flexible infer-\nence through conceptual associations mediated by distributed cortical networks. These mechanisms suggest\nthat artificial memory architectures should prioritize associative rather than positional indexing and support\nsimilarity-based retrieval that mirrors biological pattern completion processes .\n6\n\n--- Page 7 ---\nCross-Modal Integration and Binding. Biological memory systems demonstrate sophisticated cross-\nmodal integration capabilities essential for unified cognitive processing (Nyhus & Curran, 2010; Shi et al.,\n2023). Theta and gamma oscillations enable interaction between cortical structures and the hippocampus for\nencodingandretrievalofepisodicmemories, wherecorticalgammaoscillationsbindrelevantstimulusfeatures\nfor perceptual representations and gamma phase synchronization between cortical and hippocampal neurons\nprovidesthemechanismforencodingdiversecorticalinformationintohippocampalrepresentations(Nyhus&\nCurran, 2010). Evidence from spatial decision-making tasks reveals that hippocampal-prefrontal interactions\nshow maximum coherence during cross-modal binding, with theta rhythm dynamically modulating neurons\nin both regions (Tavares & Tort, 2022). Cross-modal prediction is supported by indirect pathways mediated\nby higher-order areas that receive convergent sensory inputs (Shi et al., 2023), suggesting that artificial\nmemory systems should incorporate mechanisms for binding information across modalities through oscillatory\ncoordination .\nThese computational principles collectively point toward memory-centered cognitive architectures where\nmemory systems serve as the substrate for all cognitive operations rather than passive storage devices.\nThe memory-centered cognition perspective places an active association substrate at the heart of cognition,\nmaking prediction and priming based on prior experience fundamental aspects of processing. Understanding\nthese principles provides the foundation for designing artificial memory systems that move beyond static\nstorage toward dynamic, adaptive, and context-aware memory architectures that can support the flexible,\nhierarchical, and associative processing characteristic of biological cognition.\n3 Taxonomy of Memory-Augmented Transformers\nMemory-augmented Transformers aim to overcome the fixed-context and static-knowledge constraints of\nstandard models by drawing inspiration from the dynamic nature of human memory. This section presents\na taxonomy of existing architectures along three dimensions: functional objectives, memory types, and\nintegration technique (Figure 2). We relate these categories to biological memory principles to show how\nthey help bridge the gap between current Transformers and human-like cognition.\n3.1 Categorization by Functional Objectives\nMemory-augmented Transformers address fundamental AI challenges, each mapped to a distinct functional\nobjective:\nTemporal Context Extension . Transformers struggle to process sequences beyond a fixed window, unlike\nthe human brain\u2019s ability to integrate experiences over long timescales. The evolution of temporal context\nextension reveals a clear trajectory from static windowing mechanisms toward sophisticated, biologically-\ninspired adaptive memory systems.\nSliding Window Attention (SWA) (Beltagy et al., 2020) establishes the foundational approach to linear-\ncomplexity context extension, where each token attends to a fixed window of wneighboring tokens, achiev-\ningO(n.w)complexity while maintaining parallelization. However, SWA operates as a static sensory buffer\nwithout adaptive selection or contextual awareness, limiting its effectiveness for complex temporal depen-\ndencies.\nThe field has witnessed a clear evolutionary trajectory from this static windowing toward adaptive, memory-\naugmented mechanisms that progressively incorporate working memory principles. ABC (Attention with\nBounded-Memory Control) (Peng et al., 2021) transforms static windowing through learned, contextualized\ncontrol strategies that dynamically determine token retention within fixed memory budgets. Transformer-\nFAM (Hwang et al., 2024) introduces feedback attention loops creating sustained activations across unlimited\ncontexts, effectively transforming static windows into dynamic working memory systems. NAMMs (Cetin\net al., 2025) employ STFT spectrogram analysis with genetic algorithms to evolve attention patterns for\nzero-shot cross-modal transfer, while AdaTape (Xue et al., 2023) extends this through adaptive tape tokens\nthat dynamically adjust sequence content and computational allocation. ATLAS (Behrouz et al., 2025) rep-\nresents the culmination of this trend, adding sophisticated memory mechanisms after each sliding window\nthrough the Omega rule and polynomial feature mapping, achieving super-linear memory capacity.\n7\n\n--- Page 8 ---\nTaxonomy\nof Memory-\nAugmented\nTransformers\nCategorization\nby Integration\nTechniques\nAssociative Mem-\nory IntegrationARMT (Rodkin et al., 2024), AiT (Sun et al., 2023), MemReasoner (Ko et al., 2024)Gated Control\nMechanismsTitans (Behrouz et al., 2024), RA-DT (Schmied et al., 2024), MeMOTR\n(Gao & Wang, 2023), NAMMs (Cetin et al., 2025), RMoE (Qiu et al., 2024)Attention-\nBased FusionMemformer (Wu et al., 2020), EMAT (Wu et al., 2022b), TransformerFAM (Hwang et al., 2024),\nLongMem (Wang et al., 2023), MATTER (Lee et al., 2024), Memorizing Transformer (Wu et al., 2022a)Categorization\nby memory types\nHybrid and Multi-scaleLM2 (Kang et al., 2025b), Titans (Behrouz et al., 2024), MATTER (Lee et al., 2024), AT-\nLAS (Behrouz et al., 2025), NAMMs (Cetin et al., 2025), MemGPT (Packer et al., 2023)Explicit StorageMemformer (Wu et al., 2020), EMAT (Wu et al., 2022b), MATTER (Lee et al., 2024), MemGPT (Packer\net al., 2023), CDMem (Gao et al., 2025), A-MEM (Xu et al., 2025), Memory Layers at Scale (Berges\net al., 2024), Mem0 (Chhikara et al., 2025), Think-in-Memory (Liu et al., 2023), MemLLM (Modarressi\net al., 2024), MemLong (Liu et al., 2024), MemoryLLM Wang et al. (2024b), M+ Wang et al. (2025c)State-BasedTransformer-XL (Dai et al., 2019), Compressive Transformer (Rae et al., 2019), HMT (He et al.,\n2024a), TransformerFAM (Hwang et al., 2024), WorkMATe (Kruijne et al., 2021), MemBART (Wu\n& Yu, 2022), Memformer (Wu et al., 2020), zip2zip Geng et al. (2025), RMoE (Qiu et al., 2024)Parameter-EncodedDSI (Tay et al., 2022), Schr\u00f6dinger\u2019s Memory (Wang & Li, 2024), Transformer-Squared (Sun et al.,\n2025), Titans (Behrouz et al., 2024), ATLAS (Behrouz et al., 2025), Peripheral Memory Zhai et al. (2025)Categorization\nby Functional\nObjectives\nTask-Specific\nSkill AcquisitionMeMOTR (Gao & Wang, 2023), MALT Diffusion (Yu et al., 2025), MemBART (Wu & Yu, 2022)Knowledge IntegrationEMAT (Wu et al., 2022b), MATTER (Lee et al., 2024), DSI (Tay et al., 2022),\nMemory3(Yang et al., 2024), HippoRAG (Guti\u00e9rrez et al., 2024), RETRO\n(Borgeaud et al., 2022), CDMem (Gao et al., 2025) , MemoryOS Kang et al. (2025a)Reasoning En-\nhancementMemReasoner (Ko et al., 2024), MATTER (Lee et al., 2024), Memoriz-\ning Transformer (Wu et al., 2022a), ARMT (Rodkin et al., 2024), LM2\n(Kang et al., 2025b), SAM (Le et al., 2020), ATLAS (Behrouz et al., 2025)Out-of-Distribution\nLearning & AdaptationNAMMs (Cetin et al., 2025), RA-DT (Schmied et al., 2024), Transformer-\nSquared (Sun et al., 2025), AdaTape (Xue et al., 2023), Titans (Behrouz\net al., 2024), ATLAS (Behrouz et al., 2025), zip2zip Geng et al. (2025)Temporal Con-\ntext ExtensionTransformer-X (Dai et al., 2019), Compressive Transformer (Rae et al., 2019), Memformer (Wu\net al., 2020), LongMem (Wang et al., 2023), MemLong (Liu et al., 2024), EM-LLM (Fountas et al.,\n2024), ARMT (Rodkin et al., 2024), MemWalker (Chen et al., 2023), Memorybank (Zhong et al.,\n2024a), MemoryLLM Wang et al. (2024b), M+ Wang et al. (2025c), R3mem Wang et al. (2025b)\nFigure 2: Taxonomy of Memory-Augmented Transformers.\nBuilding upon these adaptive windowing foundations, practical implementations began with simple KV\ncaching mechanisms. Transformer-XL (Dai et al., 2019) introduced the concept of caching key-value pairs\nfrom previous segments with relative positional encoding, where the cache itself functions as the memory,\nstoring compressed representations of past context to extend processing beyond fixed windows. This estab-\nlished the fundamental principle that memory in transformers is essentially intelligent caching. Successive\napproaches focused on making this cache more selective and efficient at storing important information. Com-\npressive Transformer (Rae et al., 2019) enhanced the basic KV cache through vector compression, boosting\ntemporal range by 38% by intelligently compressing older cached states rather than simply discarding them.\nHowever, it still operated under fixed storage constraints, requiring memory eviction strategies. Memo-\nryLLM Wang et al. (2024b) elevated the idea by adding a learnable write-gate, compression-on-evict, and\na neural router that selects the top-k relevant keys, enabling \u224820 k-token context with near-constant com-\npute. M+ Wang et al. (2025c) then removes this 20 k ceiling: it splits the cache into a small on-GPU\nworking store and a large CPU-resident long-term bank, coordinated by a co-trained retriever and read-\nwrite scheduler. The hierarchy preserves the compress-on-evict principle yet sustains coherent generation\nacross >160 k tokens while adding <3% throughput overhead. R3mem Wang et al. (2025b) introduces a\nreversible compression architecture that enables bidirectional transformation between raw context and com-\npressed memory representations through hierarchical chunking across multiple semantic levels\u2014segmenting\ncontent from paragraphs to sentences to sub-sentence units\u2014ensuring both efficient compression and faith-\nful reconstruction of long contexts while maintaining semantic coherence across compression-decompression\ncycles.\nMemformer (Wu et al., 2020) represented a breakthrough by decoupling computation from memory through\nsimilarity-based cache management\u2014the cache became truly adaptive, updating based on content relevance\nrather than simple temporal recency. Its MRBP optimization cut training memory costs by 55% by learning\n8\n\n--- Page 9 ---\nwhich cached representations were most valuable to retain. LongMem (Wang et al., 2023) and MemLong\n(Liu et al., 2024) further refined cache intelligence: LongMem freezes the backbone LLM and uses a trainable\nSideNet to selectively retrieve and fuse the most relevant cached key-value pairs from a growing memory\nbank, while MemLong employs Retrieval Causal Attention to actively prune less important cached entries,\ndemonstrating that the cache can learn what to forget as well as what to remember.\nEM-LLM (Fountas et al., 2024) represents the pinnacle of intelligent caching through episodic memory seg-\nmentation. It uses Bayesian surprise detection to partition the cache into meaningful episodes, enabling\nretrieval that combines both semantic similarity and temporal contiguity across sequences up to 10 million\ntokens. ARMT (Rodkin et al., 2024) scales this concept to 50 million tokens through Hopfield-inspired\nassociative caching with explicit erase operations, while MemWalker (Chen et al., 2023) creates hierarchi-\ncal cache structures using trees of text summaries, and Memorybank (Zhong et al., 2024a) implements\ncognitively-inspired cache decay following human memory patterns like the spacing effect.\nThis progression mirrors the hierarchical integration of biological memory systems, where simple sensory\nbuffering (SWA, basic KV caching) evolves into sophisticated working memory mechanisms (adaptive win-\ndowing, intelligent cache management) that balance immediate processing needs with longer-term contextual\nunderstanding, moving toward truly cognitive attention systems that integrate multiple timescales of tem-\nporal context.\nOut-of-Distribution (OOD) Learning and Adaptation . Memory-augmented Transformers address the\nchallenge of adapting to novel data distributions while preserving performance on familiar content through\nsurprise-driven mechanisms that mirror biological memory systems\u2019 ability to encode novel experiences while\nmaintaining stable knowledge representations (Barry & Gerstner, 2024; Sinclair et al., 2021; Frank et al.,\n2022).\nSurprise-driven adaptation forms the core of effective OOD learning. EM-LLM (Fountas et al., 2024) demon-\nstrates this through Bayesian surprise detection and graph-theoretic boundary refinement to segment se-\nquences into episodic events. This training-free approach automatically detects distribution shifts, creating\ndistinct memory episodes for novel patterns while preserving performance on familiar content, embody-\ning how novelty detection enables rapid adaptation to unexpected domains without compromising existing\nknowledge.\nTitans (Behrouz et al., 2024) advance surprise-driven adaptation through prediction error gating, where KL\ndivergence thresholds at the single-token level determine when memory updates occur. This token-by-token\nsurprise detection enables fine-grained, test-time learning without parameter modification, allowing models\nto selectively memorize novel information while avoiding interference with established knowledge. ATLAS\n(Behrouz et al., 2025) similarly employs surprise signals through the Omega rule at the local context level,\nusing sliding windows to determine which multi-token contexts warrant long-term memorization based on\nprediction error magnitudes across sliding windows rather than individual tokens. Dynamic Input Pruning\n(Federici et al., 2024) achieves zero-parameter adaptation through magnitude-based pruning of MLP activa-\ntions per token, implementing a predictor-free strategy for real-time efficiency improvements without model\nretraining.\nzip2zip Geng et al. (2025) demonstrates adaptive tokenization as a novel OOD adaptation strategy, dynami-\ncally expanding vocabulary at inference time through compression-based token merging that enables models\nto efficiently process unfamiliar token patterns and domains without retraining, achieving significant latency\nimprovements while maintaining adaptability to new linguistic distributions.\nEvolutionary and adaptive mechanisms enable cross-domain generalization without domain-specific train-\ning. NAMMs (Cetin et al., 2025) demonstrate evolutionary optimization of attention patterns using genetic\nalgorithms and STFT spectrogram analysis to evolve token retention policies for zero-shot cross-modal trans-\nfer. AdaTape (Xue et al., 2023) extends adaptive allocation through elastic input sequences with adaptive\ntape tokens that dynamically adjust sequence content and computational allocation based on problem com-\nplexity. RA-DT (Schmied et al., 2024) combines episodic memory with surprise-based pruning to retain\nhigh-error experiences, boosting multitask efficiency by 40% while mimicking dopaminergic learning mech-\nanisms. Transformer-Squared (Sun et al., 2025) encodes procedural expertise directly into parameter space\n9\n\n--- Page 10 ---\nusing SVD, dynamically blending expert vectors during inference to reach 90% accuracy on unseen tasks,\ndespite a 15% latency overhead. Dutta & Sra (2024) extends the Memformer architecture to procedural\ncomputation by storing and combining past optimization gradients with learned coefficients, achieving 98%\nconvergence accuracy on OOD tasks through trial-and-error learning patterns that mirror biological motor\nlearning systems.\nThese approaches collectively demonstrate that effective OOD adaptation requires selective memory up-\ndating mechanisms that balance novelty detection with stability preservation, enabling memory-augmented\nTransformers to achieve flexible adaptation characteristics of biological memory systems while maintaining\ncomputational tractability and avoiding catastrophic forgetting.\nReasoning Enhancement . Extended context windows fundamentally enhance reasoning capabilities by\nproviding access to larger knowledge bases and longer chains of inference (Yang et al., 2025). However, the\nrelationship between context length and reasoning performance is not simply linear - it requires sophisticated\nmemory mechanisms to maintain coherence across extended sequences, as standard attention mechanisms\nstruggle with long-range dependencies and coherence degradation in extended contexts (Press et al., 2021).\nMemory-augmented Transformers address these challenges by integrating scattered information over ex-\ntended contexts for multi-hop inference and relational reasoning. Several approaches demonstrate significant\nimprovements in reasoning performance through different memory architectures. MemReasoner (Ko et al.,\n2024) bridges encoders and decoders using a temporally-aware memory module with bidirectional GRUs\nand iterative updates, improving multi-hop QA by 18%. MATTER (Lee et al., 2024) unifies unstructured\ntext and QA pairs into neural memories, using a cross-encoder to link questions to relevant data, boosting\nthroughput by 100 \u00d7and HotpotQA accuracy by 12%.\nFor tasks requiring extensive retrieval and pattern completion, associative memory approaches show particu-\nlar promise. The Memorizing Transformer (Wu et al., 2022a) uses a kNN-retrievable memory to dynamically\nintegratedistantcontextfortasksliketheoremprovingandcodegeneration, scalingto262Ktokenswhileout-\nperforming baselines in long-range reasoning, mirroring hippocampal episodic retrieval for problem-solving.\nARMT (Rodkin et al., 2024) scales reasoning across 50 million tokens with associative memory blocks for\npattern completion and interference mitigation, echoing hippocampal mechanisms. Self-Attentive Associa-\ntive Memories (SAM) (Le et al., 2020) uses dual memory units and outer-product attention for updating\nitem and relationship memories, improving performance on graph and geometric reasoning tasks such as the\nTraveling Salesman Problem and shortest path finding.\nGated memory mechanisms provide another effective approach to reasoning enhancement. LM2 (Kang\net al., 2025b) adds memory modules with gated mechanisms to each decoder layer, outperforming standard\nTransformers on multi-hop reasoning over 128k-token contexts. ATLAS (Behrouz et al., 2025) shows that\noptimal context memorization enables complex reasoning by learning which historical information remains\nrelevant for current inferences.\nAlternative architectures explore hierarchical reasoning approaches that prioritize computational depth over\nextended context. HRM (Wang et al., 2025a) addresses reasoning depth through hierarchical convergence\nusingcoupledrecurrentmodulesthatachieveenhancedcomputationaldepthforproblemsrequiringextensive\nsearch and backtracking. For resource-constrained settings, Memory-R+ (Le et al., 2025) enhances reasoning\nin tiny LLMs (\u22641B parameters) through dual episodic memory modules that provide intrinsic rewards for\nexploration and exploitation, achieving 2-14% performance improvements.\nThe key insight is that reasoning benefits from both quantity and quality of accessible context. Longer con-\ntexts provide more potential information, but sophisticated memory mechanisms are required to selectively\nattend to relevant information while avoiding interference from irrelevant details.\nKnowledge Integration . Knowledge Integration encompasses the synthesis, storage, and retrieval of\ndiverse information types into unified representations that support reasoning and generation. This process\nrequires sophisticated indexing mechanisms that enable models to dynamically combine information from\nmultiple sources for context-aware generation.\n10\n\n--- Page 11 ---\nRetrieval-Augmented and Hierarchical Approaches demonstrate efficient knowledge incorporation strategies.\nRETRO (Borgeaud et al., 2022) combines frozen BERT retrievers with differentiable cross-attention, achiev-\ning GPT-3 performance with 25 \u00d7fewer parameters through access to 2 trillion token databases. CDMem\n(Gao et al., 2025) implements hierarchical three-stage encoding, i.e., expert, short-term, and long-term,\nthrough graph-structured, context-dependent indexing, achieving 85.8% success on ALFWorld and 56.0%\non ScienceWorld by enabling multilevel knowledge recall tailored to current contexts.\nHeterogeneous Memory Integration unifies diverse knowledge formats within a single architecture. EMAT\n(Wu et al., 2022b) encodes millions of QA pairs into key-value memory using fast MIPS for sub-millisecond\nquerying, improving Natural Questions performance from 25.8 to 44.3 EM while maintaining 1000 queries/s\nthroughput. MATTER (Lee et al., 2024) integrates both unstructured and semi-structured sources into\ntype-agnostic neural memories, achieving 100 \u00d7throughput improvement over conventional retrieve-and-read\nmodels.\nParameter-Encoded and Brain-Inspired Systems explore direct knowledge embedding and neurobiological\narchitectures. Memory3(Yang et al., 2024) converts a textual knowledge base into a memory bank which\ncan be seen as a bank of sparse retrievable parameters, enabling smaller language models to match the\nperformance of bigger models, as well as reducing hallucinations and increasing factuality. DSI (Tay et al.,\n2022) encodes document corpora directly into model weights, enabling direct query-to-document mapping.\nHippoRAG (Guti\u00e9rrez et al., 2024) brings knowledge integration to RAG systems through the construction\nof a concept graph inspired by the hippocampus, outperforming RAG in multi-hop QA by up to 20% while\nbeing 10-30 \u00d7cheaper and 6-13 \u00d7faster.\nMemoryOS Kang et al. (2025a) introduces an operating system-inspired hierarchical memory architecture for\nAI agents, featuring three-tier storage (short-term for immediate context, mid-term for recent interactions,\nand long-term for persistent personal memory) managed through four core modules (Storage, Updating,\nRetrieval, and Generation), which enable evolutionary adaptation via heat-based segment prioritization and\ndialogue-chain FIFO mechanisms; this results in a 49.11% F1 score improvement on long-term conversational\nbenchmarks like LoCoMo, outperforming baselines by enhancing factual consistency and personalization in\nextended dialogues.\nThese approaches demonstrate that effective knowledge integration requires semantic organization, efficient\naccess patterns, and scalable architectures that handle massive knowledge bases while maintaining precision.\nThe convergence of retrieval-augmented methods, hierarchical encoding, and neurobiologically-inspired de-\nsigns enables memory-augmented Transformers to bridge static parametric models with dynamic knowledge\nsystems capable of large-scale, multi-format integration.\nTask-Specific Skill Acquisition . Task-Specific Skill Acquisition enables models to learn and apply pro-\ncedural knowledge for specialized tasks-such as object tracking, video generation, or dialogue, by encoding\noperations for robust, context-aware performance. Notable architectures include MeMOTR (Gao & Wang,\n2023), which uses object-specific long-term memory with exponential decay and confidence-based updates\nfor multi-object tracking. MALT Diffusion (Yu et al., 2025) employs recurrent attention and memory vectors\nto generate temporally coherent videos over long durations. In dialogue, MemBART (Wu & Yu, 2022) pre-\nserves memory states across turns, enhancing response quality. These models demonstrate that specialized\nmemory mechanisms, whether persistent, episodic, or stateful, are essential for robust skill acquisition and\ndeployment, echoing the compartmentalization of procedural memory in biological systems.\n3.2 Categorization by memory types\nMemory-augmented Transformers can be systematically differentiated by memory types, each offering\ndistinct computational and cognitive properties: parameter-encoded, state-based, explicit storage, and\nhybrid/multi-scale systems.\nParameter-Encoded Memory . Parameter-encoded memory systems store knowledge directly within\nmodel weights, analogous to synaptic consolidation in biological systems where knowledge becomes dis-\ntributed across neural connections. This approach offers fundamental advantages including immediate access\nwithout external retrieval operations and unified processing where memory and computation share the same\n11\n\n--- Page 12 ---\nparameter space. However, capacity constraints emerge as a critical limitation since memory capacity is\nbounded by the number of parameters available for knowledge storage.\nTraining-time parameter encoding provides stable, consolidated knowledge but lacks adaptability. DSI (Dif-\nferentiable Search Index) (Tay et al., 2022) revolutionizes retrieval by encoding entire document corpora di-\nrectly into standard Transformer parameters, transforming traditional retrieval into a generative task where\nmodels learn direct query-to-document mappings through the existing attention and feedforward mecha-\nnisms. Schr\u00f6dinger\u2019s Memory (Wang & Li, 2024) reveals the latent memory capabilities of large language\nmodels, demonstrating that LLMs can reconstruct complete datasets from minimal contextual cues through\nparameter-encoded associations formed during training. The key insight is that memory exists in a \"superpo-\nsition\" state, remaining hidden until specific contextual triggers activate associative recall patterns, much like\nhuman memory retrieval from partial cues. Memory3(Yang et al., 2024) converts textual knowledge bases\ninto explicit memory banks functioning as sparse retrievable parameters, implemented through specialized\nembedding layers and sparse attention mechanisms. The system uses aggressive sparsification techniques\nand two-stage pretraining to efficiently store 1.1 \u00d7108text chunks within modified feedforward networks\nthat enable smaller language models to match larger model performance.\nTest-timeparameterlearningrepresentsarevolutionaryadvancewhereparametersadaptdynamicallyduring\ninference, addressing the fundamental limitation of static knowledge storage. Titans (Behrouz et al., 2024)\nuses MLP-based memory with KL divergence thresholds for surprise-driven, real-time parameter updates,\nmaintaining stability via gating mechanisms that prevent catastrophic interference. ATLAS (Behrouz et al.,\n2025) enhances test-time learning by expanding MLP capacity through polynomial feature mapping and\nemploys the Omega rule for sliding window optimization, achieving super-linear memory growth without\ntraditional gradient descent. Transformer-Squared (Sun et al., 2025) enables real-time task adaptation\nby encoding procedural expertise directly into parameter space using SVD decomposition of feedforward\nlayers, dynamically blending expert vectors during inference through specialized MLP mixing networks.\nSimilarly, Peripheral Memory Zhai et al. (2025) introduces a CPU-RAM analogous architecture where LLMs\nfunction as processors interfacing with parameter-encoded memory banks modeled through Kolmogorov-\nArnoldNetworks, enablingdynamicmemoryoperationscontrolledbyinternalmodelstateswhilemaintaining\ndirect integration with the model\u2019s parameter space. This approach demonstrates how parameter-encoded\nsystems can capture and recombine procedural knowledge using adaptive feedforward architectures rather\nthan just declarative facts.\nThe evolution from static parameter encoding (DSI, Schr\u00f6dinger\u2019s Memory) to dynamic parameter learning\n(Titans, ATLAS, Transformer-Squared) represents a paradigm shift toward adaptive parameter-encoded\nsystems that combine the efficiency advantages of parameter encoding with flexible adaptation capabilities.\nWhile training-time approaches provide stable knowledge consolidation, test-time parameter learning enables\nreal-time adaptation with capacity enhancement techniques like polynomial feature mapping addressing\nfundamental scalability constraints. This progression points toward future architectures where parameter-\nencoded memory becomes truly dynamic, supporting both stable knowledge consolidation and adaptive\ncapacity expansion during deployment.\nState-Based Memory . State-based memory maintains information through persistent activations or hid-\nden states that carry forward across processing steps, fundamentally differing from parameter-encoded ap-\nproaches in that memory resides in dynamic activations rather than static weights. This approach mirrors\nbiologicalworkingmemorysystemswhereinformationismaintainedthroughsustainedneuralfiringpatterns,\nenabling temporal continuity and context preservation across extended sequences.\nTransformer-XL (Dai et al., 2019) pioneered this approach through segment-level recurrence, caching hidden\nstates from previous segments with relative positional encoding to extend context beyond fixed windows.\nWhile achieving substantial improvements in perplexity and long-range dependency capture, this method\nrequires significant memory resources as cached states accumulate. Compressive Transformer (Rae et al.,\n2019) addressed memory intensity through compressed state buffering, maintaining recent states in full\nresolution while compressing older memories using learned functions, extending temporal range by 38% and\nreflectingbiologicalmemory\u2019stendencytoretainvividrecentexperienceswhileabstractingolderinformation.\nHierarchical Memory Transformer (HMT) (He et al., 2024a) extends this idea by layering three progressively\n12\n\n--- Page 13 ---\ncoarser caches\u2014token-, chunk-, and segment-level\u2014on topof the basic recurrent buffer, allowing 100K-token\nstreams on a single GPU while hierarchically pruning stale activations and keeping recent ones in full detail.\nAdvanced state-based mechanisms have emerged with sophisticated memory management capabilities.\nTransformerFAM (Hwang et al., 2024) introduces feedback attention loops where each layer attends to\nits own latent representations from previous time steps, creating internal working memory that enables in-\ndefinite sequence processing with O(L)complexity. This sustained activation mechanism transforms static\nattention layers into dynamic working memory systems capable of maintaining coherent representations\nacross arbitrarily long sequences. WorkMATe (Kruijne et al., 2021) implements biologically-inspired gated\nmemory circuits controlled by internal actions through reinforcement learning, successfully handling hierar-\nchical tasks like 12-AX where multiple context levels must be maintained simultaneously, demonstrating how\nselective gating enables multiple independent representations within shared activation spaces. RMoE Qiu\net al. (2024) extends state-based memory concepts to mixture-of-experts routing, where GRU-maintained\nhidden states capture routing history across consecutive layers, enabling each routing decision to leverage\naccumulated routing patterns from previous layers for improved expert selection and utilization.\nSpecialized applications further demonstrate state-based memory\u2019s versatility. MemBART (Wu & Yu, 2022)\nappliespersistentmemorystates for dialoguemodeling, preserving conversationcontext across multipleturns\nto enable coherent long-term interactions. Memformer (Wu et al., 2020) employs a unified memory approach\ncombining internal state representations with external memory banks, using Memory Replay Backpropaga-\ntion (MRBP) to optimize memory usage and reduce training costs by 55%, representing a hybrid between\nstate-based and explicit storage approaches. The Hierarchical Reasoning Model (HRM) Wang et al. (2025a)\nimplements dual recurrent modules operating at different timescales\u2014high-level for abstract planning and\nlow-level for detailed computations\u2014achieving enhanced effective depth of deep computation while main-\ntaining training stability, demonstrating near-perfect performance on complex reasoning tasks with only 27M\nparameters.\nState-based memory systems offer fundamental computational advantages including temporal continuity\nthrough persistent activations, efficient information propagation across time steps, and seamless integration\nwithsequentialprocessingarchitectures. Thesesystemsexcelatmaintainingcoherentinformationflowwithin\nprocessing sequences, enabling models to preserve contextual representations across multiple computation\nsteps without external storage overhead.\nHowever, state-based approaches face inherent architectural constraints: memory capacity is fundamentally\nbounded by hidden state dimensions, creating potential bottlenecks for complex information storage. In-\nterference between heterogeneous information types stored within shared activation spaces poses additional\nchallenges, as different memory contents must compete for the same representational resources. Further-\nmore, the computational overhead of maintaining and updating persistent states throughout processing can\nbecome substantial for long sequences.\nExplicit Storage Memory . Explicit storage memory employs external modules for scalable information\nstorageandretrieval, maintainingpersistentmemorybanksthatsurvivebeyondindividualinferencesessions.\nUnlike parameter-encoded memory that stores knowledge within model weights or state-based memory that\nmaintains information through activations, explicit storage systems utilize dedicated external storage mod-\nules that can be accessed, updated, and scaled independently of the core model architecture, analogous to\nhippocampal indexing where sparse representations point to distributed memory traces.\nFoundational approaches established core principles through dedicated external modules. Memformer (Wu\net al., 2020) pioneered fixed-size external key-value stores with similarity-based retrieval, demonstrating\nefficient integration with Transformer architectures. EMAT (Wu et al., 2022b) implements compact neural\nmemories for structured knowledge storage with fast retrieval capabilities, while MATTER (Lee et al.,\n2024) integrates heterogeneous data sources into unified external memory frameworks, achieving orders of\nmagnitude throughput improvements while maintaining type-agnostic storage capabilities.\nAdvanced external storage systems introduce sophisticated organization and management strategies.\nMemGPT (Packer et al., 2023) implements OS-inspired hierarchical storage with main context and archival\nstores managed through function calls, enabling unbounded context through intelligent paging policies. CD-\n13\n\n--- Page 14 ---\nMem (Gao et al., 2025) exemplifies graph-structured external storage through context-dependent indexing\nthat organizes agent experiences into comprehensive external knowledge bases. A-MEM (Xu et al., 2025)\nadvances this through automatically generated and evolving memory notes that form dynamic external\nknowledge graphs capturing semantic relationships over time.\nSpecialized applications demonstrate external storage versatility across different domains. Memory Layers\nat Scale (Berges et al., 2024) embeds external key-value slots within Transformer layers using product-\nkey lookup for web-scale deployment, while Mem0 (Chhikara et al., 2025) targets production environments\nthrough external memory systems that blend vector embeddings with graph-structured representations for\npersistent user-specific memory. Think-in-Memory (Liu et al., 2023) and MemLLM (Modarressi et al., 2024)\nconstruct external triplet memory systems storing subject-object-relation structures, enabling models to\nquery relationships through external memory rather than parameter-encoded associations. MemLong (Liu\net al., 2024) demonstrates context extension by retrieving past embeddings from external storage systems,\nhandling up to 80K tokens while preserving core model parameters. Memory-R+ Le et al. (2025) demon-\nstrates intrinsic motivation applications where separate success and failure memory modules use kNN-based\nretrieval to compute rewards that guide reinforcement learning in tiny LLMs.\nKey distinguishing characteristics define explicit storage memory\u2019s advantages. Memory persistence en-\nables permanent knowledge banks that accumulate information across sessions, unlike temporary state-based\nmemory. Independent scalability allows external modules to expand knowledge capacity without requiring\nproportional increases in core model parameters. Structured organization predominantly stores structured\nand semi-structured data through indexing schemes including vector similarity search, graph traversal, and\nhierarchical clustering, enabling efficient access to diverse knowledge types.\nUnlike simple retrieval-augmented generation approaches, explicit storage memory-augmented Transformers\nimplement tightly integrated, differentiable memory modules that enable end-to-end optimization and so-\nphisticated memory management strategies. These systems provide seamless integration between external\nmemory operations and model computations, supporting dynamic knowledge updates and context-aware\nmemory management. Explicit storage architectures enable Transformers to scale beyond fixed context\nlimitations, support continual knowledge integration, and provide efficient retrieval for complex reasoning\napplicationsthroughpersistent, structuredexternalmemorysystemsthatevolveindependentlyofcoremodel\nconstraints.\nHybrid and Multi-Scale Memory Systems . Hybrid memory systems combine multiple memory types,\nincluding parameter-encoded, state-based, and explicit storage, within unified architectures, creating hier-\narchical memory organizations that leverage the complementary strengths of different memory mechanisms.\nThis architectural approach mirrors the brain\u2019s integration of multiple memory subsystems, where different\ntemporal scales and storage mechanisms work together to support flexible cognition.\nLM2 (Kang et al., 2025b) demonstrates sophisticated parameter-state hybrids by integrating external mem-\nory modules with learnable gates into each decoder layer, enabling dynamic coordination between internal\nrepresentations and external storage. Titans (Behrouz et al., 2024) advance this by combining state-based\nattention with parameter-encoded long-term memory modules that adapt during test time, while MATTER\n(Lee et al., 2024) represents parameter-explicit hybrids that encode diverse knowledge into model weights\nwhile maintaining external retrieval capabilities. These examples demonstrate how different memory types\ncan coexist within single architectures to handle both immediate processing needs and long-term knowledge\naccess.\nThese systems establish memory hierarchies based on temporal characteristics, creating multi-tiered archi-\ntectures where fast state-based memory handles immediate context, medium-speed explicit storage manages\nsession-persistent information, and slow parameter-encoded memory provides consolidated knowledge foun-\ndations. MemGPT (Packer et al., 2023) exemplifies this through OS-inspired memory management that\ncoordinates working context (state-based) with archival storage (explicit) through learned paging policies.\nAdvanced hybrid systems like ATLAS (Behrouz et al., 2025) and NAMMs (Cetin et al., 2025) implement\ndynamic memory allocation that adaptively distributes memory resources across different types based on\ntask demands and surprise signals. ATLAS uses context-aware optimization to determine when information\n14\n\n--- Page 15 ---\nshould transition between memory stores, while NAMMs employ evolutionary algorithms to optimize mem-\nory allocation patterns across modalities. This adaptive coordination demonstrates that effective memory\nsystems require intelligent arbitration between memory types rather than static allocation schemes.\nThe evolution toward hybrid architectures represents a fundamental shift from single-memory-type systems\ntoward cognitively-inspired memory ecosystems that mirror the distributed, hierarchical organization of\nbiological memory. These systems achieve computational flexibility by combining the immediate access of\nparameter-encoded memory, the temporal continuity of state-based memory, and the scalable capacity of\nexplicit storage within unified frameworks that can dynamically adapt to diverse cognitive demands.\n3.3 Categorization by Integration Techniques\nThe effectiveness of memory-augmented Transformers depends not only on what they remember, but also\non how stored knowledge is integrated into ongoing computations. Integration techniques determine how\nretrieved information influences model behavior and how memory states evolve, reflecting the sophisticated\ncoordination mechanisms found in biological memory systems.\nAttention-Based Fusion remains the primary method for integrating memory content, enabling dynamic\nselection and weighting of stored information. Memformer (Wu et al., 2020) pioneered cross-attention be-\ntween layer activations and external memory banks, gating semantically salient tokens much like thalamo-\ncortical loops filter relevant information in the brain. EMAT (Wu et al., 2022b) accelerates this approach\nby issuing retrieval queries at early layers and propagating key-value pairs through decoder stages, achieving\nmillisecond-scale throughput for real-time applications. TransformerFAM (Hwang et al., 2024) advances\nfusion through feedback attention loops within each layer, creating internal working memory that supports\nindefinitely long contexts without external cache management. LongMem (Wang et al., 2023) introduces\nhybrid fusion via its SideNet module, which decouples memory retrieval from backbone updates while adap-\ntively blending live inputs with cached representations through residual connections. MATTER (Lee et al.,\n2024)demonstratesheterogeneousfusionbyencodingdiversecontenttypesintofixed-lengthneuralmemories\naccessedthroughuniversalattentionheads, whiletheMemorizingTransformer(Wuetal.,2022a)implements\nkNN-based attention over rolling buffers to approximate human-like recency bias with logarithmic complex-\nity.\nGated Control Mechanisms implement neuromodulatory-inspired regulation of memory updates and re-\ntention, mirroring how biological systems selectively encode and maintain information. Titans (Behrouz\net al., 2024) employs surprise-driven writes triggered by KL divergence thresholds, mimicking nore-\npinephrine\u2019s role in novelty detection and memory consolidation. RA-DT (Schmied et al., 2024) combines\nepisodic memory with adaptive forgetting gates based on statistical surprise, reducing catastrophic forgetting\nby40%inmulti-taskreinforcementlearningscenarios. MeMOTR(Gao&Wang,2023)integratesexponential\ndecay with confidence-driven pruning for object tracking, replicating striatal pathway dynamics that bal-\nance stability with adaptability. NAMMs (Cetin et al., 2025) takes an evolutionary approach, using genetic\nalgorithms to evolve token retention policies that balance stability and plasticity through GABAergic-like\ninhibition mechanisms. RMoE (Qiu et al., 2024) demonstrates how GRU-based gated control can enhance\nrouting efficiency in Mixture-of-Experts architectures by leveraging historical routing patterns across layers,\nestablishing dependencies between routing decisions to improve parameter efficiency and expert selection\ndiversity.\nAssociative Memory Integration enables content-addressable recall and efficient pattern completion\nacross large contexts, shifting from positional to semantic indexing. ARMT (Rodkin et al., 2024) implements\nHopfield-inspired associative blocks for O(1)retrieval over 50 million tokens, directly mirroring hippocampal\nCA3 circuits\u2019 role in relational memory and pattern completion. Associative Transformer (AiT) (Sun et al.,\n2023) employs low-rank memory priors as attractors within a global workspace architecture, mimicking\ncortical column dynamics and distributed representation schemes. MemReasoner (Ko et al., 2024) enhances\nassociative integration through bidirectional GRUs that support iterative read-update cycles, maintaining\ncoherence across long documents through sustained memory interactions.\nThese integration strategies collectively represent a paradigm shift from fixed positional indexing toward\ncontent-sensitive memory access that bridges artificial attention mechanisms with neural memory systems.\n15\n\n--- Page 16 ---\nByimplementingbiologically-inspiredfusion,gating,andassociativemechanisms,memory-augmentedTrans-\nformers achieve more flexible and context-aware information integration that approaches the adaptive capa-\nbilities of human cognition. The convergence of these techniques enables models to dynamically coordinate\nmultiple memory systems while maintaining computational efficiency and biological plausibility.\n4 Mechanisms of Memory Operations\nMemory-augmented Transformers overcome fixed-context limits of standard architectures by integrating\nneuroscience and engineering advances for dynamic, scalable memory. This section reviews core mechanisms,\ni.e., reading, writing, forgetting, capacity optimization, and self-management/adaptation, highlighting key\ntechniques and representative models from the recent literature.\nRead Operations. Early neural memories such as the Neural Turing Machine (Graves et al., 2014), DNC\n(Graves et al., 2016), and Kanerva Machines (Wu et al., 2018) introduced content-based addressing, but\nmodern memory-augmented Transformers refine the read step with specialised retrieval mechanisms tailored\nto massive stores. Memory Layers at Scale (Berges et al., 2024) replaces dense feed-forward blocks with\ntrainable key\u2013value layers that perform product-key lookup, giving sub-linear top-k search across billions\nof entries while preserving end-to-end differentiability. EMAT (Wu et al., 2022b) shows that maximum-\ninner-product search can return millions of QA pairs in sub-millisecond latency, letting the model integrate\nexternal knowledge at every decoding step without harming throughput. The Memorizing Transformer (Wu\net al., 2022a) augments attention with approximate k-nearest-neighbour queries into a continually growing\ncache, scaling recall to 262 k tokens and matching the perplexity of much larger dense models.\nAssociative designs push retrieval to constant time: ARMT (Rodkin et al., 2024) stores tokens in Hopfield-\nstyle energy basins for O(1) pattern completion over 50 M-token contexts, and AiT (Sun et al., 2023) adds\nlow-rank priors that reconstruct missing tokens from partial cues, outperforming sparse Transformers on\nrelational reasoning benchmarks. For multi-hop discourse, MemReasoner (Ko et al., 2024) iteratively re-\nreads a temporal memory with bidirectional GRUs until the readout stabilises, boosting long-document\nquestion answering. MemLong (Liu et al., 2024) couples local attention with retrieval-causal attention that\nselects semantically relevant chunks from an 80 k-token cache, maintaining single-GPU efficiency.\nMore adaptive schemes appear in CDMem (Gao et al., 2025), which navigates a graph-indexed memory to\nfetch task-specific subgraphs, and ABC (Peng et al., 2021), which learns neural policies that decide when and\nhow deeply to probe memory rather than relying on fixed heuristics. Finally, NAMMs (Cetin et al., 2025)\ndemonstrate that the attention matrix itself can encode reusable retrieval plans, enabling zero-shot read\nstrategies that transfer across modalities. Together these mechanisms move the field from uniform similarity\nsearch toward context-sensitive, learned, and even evolutionary reading policies that approach the flexibility\nof biological episodic recall.\nWrite Operations. Memory-augmented Transformers now treat writing as an active, learned decision\nrather than an unconditional overwrite. Titans (Behrouz et al., 2024) triggers a write only when prediction-\nerror\u2013derived surprise exceeds a KL-based threshold, mirroring dopamine-gated consolidation and allowing\nthe model to memorise rare events without destabilising prior knowledge. LM2 (Kang et al., 2025b) intro-\nduces per-layer input/forget/output gates around an external store, so each decoder layer decides in real\ntime how much of its state should be committed, yielding controllable long-context reasoning without extra\nfine-tuning. Memformer (Wu et al., 2020) ports LSTM-style gates into a key\u2013value memory, giving fine-\ngrained retention and erasure that stabilise sequence modelling, while MeMOTR (Gao & Wang, 2023) adds\nexponential decay plus confidence gating to keep only high-value object tracks in video streams.\nBeyond simple gating, A-MEM (Xu et al., 2025) writes \u201cmemory notes\u201d that are later linked and evolved into\na graph, creating a self-organising semantic store that grows with the agent\u2019s experience . Memory Layers\nat Scale (Berges et al., 2024) spreads writes across product-key memory shards on multiple GPUs, enabling\ncontinual learning at web scale without bottlenecks. MemBART (Wu & Yu, 2022) mitigates read\u2013write\ninterference in dialogue by running parallel attention streams and merging them through residual gates. In\nprocedural settings, Memformers (Dutta & Sra, 2024) treat past optimisation gradients as first-class mem-\nory registers, letting the model cache and reuse computation traces during new tasks. ATLAS (Behrouz\n16\n\n--- Page 17 ---\nTable 1: Mechanisms of memory operations in memory-augmented Transformers, with key techniques and\nrepresentative models.\nOperation Key Mechanism High-fidelity Representative Models\nReadContent-based addressing Neural Turing Machine (Graves et al., 2014); DNC (Graves et al.,\n2016); Kanerva Machine (Wu et al., 2018)\nSpecialised similarity search Memory Layers at Scale (Berges et al., 2024); EMAT (Wu et al.,\n2022b); Memorizing Transformer (Wu et al., 2022a)\nAssociative retrieval ARMT (Rodkin et al., 2024); AiT (Sun et al., 2023); MemRea-\nsoner (Ko et al., 2024); MemLong (Liu et al., 2024)\nAdaptive graph / policy-driven reads CDMem (Gao et al., 2025); ABC (Peng et al., 2021);\nNAMMs (Cetin et al., 2025)\nWriteSurprise / uncertainty-gated writes Titans (Behrouz et al., 2024); LM2 (Kang et al., 2025b); MeM-\nOTR (Gao & Wang, 2023)\nLSTM-style input\u2013forget gating Memformer (Wu et al., 2020); WorkMATe (Kruijne et al., 2021);\nRMoE (Qiu et al., 2024)\nConfidence-filtered updates A-MEM (Xu et al., 2025); MemBART (Wu & Yu, 2022); Memo-\nryLLM (Wang et al., 2024b); M+ (Wang et al., 2025c); Memory-\nR+ (Le et al., 2025)\nReinforcement / optimisation traces Memformers (Dutta & Sra, 2024); ATLAS (Behrouz et al., 2025)\nForgetSelective pruning MemLong (Liu et al., 2024); MeMOTR (Gao & Wang, 2023); Ti-\ntans (Behrouz et al., 2024)\nExponential decay MeMOTR (Gao & Wang, 2023); LM2 (Kang et al., 2025b)\nAdaptive (gate-controlled) decay ARMT (Rodkin et al., 2024); Memformer (Wu et al., 2020); Mem-\noryBank (Zhong et al., 2024a)\nSurprise-triggered erase Titans (Behrouz et al., 2024); EM-LLM (Fountas et al., 2024)\nTask-aware forgetting RA-DT (Schmied et al., 2024)\nCapacityLearned compression Compressive Transformer (Rae et al., 2019); MATTER (Lee et al.,\n2024); EMAT (Wu et al., 2022b); zip2zip (Geng et al., 2025)\nHierarchical chunk / tree buffers MemLong (Liu et al., 2024); LM2 (Kang et al., 2025b); Mean-\ningful Memory (Zhong et al., 2024b); M+ (Wang et al., 2025c);\nHRM (Wang et al., 2025a)\nSharded / product-key KV Memory Layers at Scale (Berges et al., 2024)\nSelf-\nManagementDynamic allocation at test time Transformer-Squared (Sun et al., 2025); NAMMs (Cetin et al.,\n2025); Titans (Behrouz et al., 2024); Peripheral Memory (Zhai\net al., 2025)\nSub-system specialisation MATTER (Lee et al., 2024); MemBART (Wu & Yu, 2022); Mem-\noryOS (Kang et al., 2025a)\nInterference control ARMT (Rodkin et al., 2024); MemReasoner (Ko et al., 2024); RA-\nDT (Schmied et al., 2024); Schr\u00f6dinger\u2019s Memory (Wang & Li,\n2024)\net al., 2025) pushes test-time learning further: its Omega rule adjusts memory weights over sliding windows\nwith polynomial feature mapping, achieving super-linear capacity growth without gradient descent. Finally,\nWorkMATe (Kruijne et al., 2021) shows that reinforcement-learned gating policies can independently open or\nclose multiple working-memory slots, supporting concurrent, interference-free storage of task rules. Collec-\ntively, these writing mechanisms shift the focus from passive storage to selective, context-aware, and scalable\nwriting, a prerequisite for lifelong, low-interference memory in Transformer systems.\nForgetting Dynamics. Effective forgetting sustains continual learning by pruning obsolete traces and\nfreeing capacity for salient information. Modern memory-augmented Transformers therefore implement\nselective, learned erase policies rather than indiscriminate decay. MemLong (Liu et al., 2024) prunes keys\nwhose retrieval counts fall below a threshold, ensuring its external cache stays focused on behaviourally\nrelevant chunks. MeMOTR (Gao & Wang, 2023) adds confidence-weighted exponential decay so unreliable\nobject tracks vanish naturally as a video unfolds. Titans (Behrouz et al., 2024), LM2 (Kang et al., 2025b),\nand Atlas (Behrouz et al., 2025) all regulate forgetting with adaptive gates: Titans couples KL-surprise with\n17\n\n--- Page 18 ---\na trainable decay factor, LM2 ties gate strength to layer-wise uncertainty, and ATLAS uses the Omega-\nrule\u2019s sliding-window optimisation\u2014effectively down-weighting contributions from tokens that lie outside a\npolynomially mapped context window, allowing new information to replace stale traces without gradient\ndescent.\nAggressive cleanup appears in ARMT (Rodkin et al., 2024), whose Hopfield memory periodically normalises\nand hard-deletes outdated vectors, preventing spurious attractors. Memformer (Dutta & Sra, 2024) mixes\nLSTM-style forget gates with memory-replay back-propagation so rarely used slots fade while important\nones are refreshed, and MemoryBank (Zhong et al., 2024a) models retention with an Ebbinghaus-shaped\ncounter that decays unless the entry is reaccessed. EM-LLM (Fountas et al., 2024) reinforces this trend by\ncoupling prediction-error spikes to simultaneous write-and-prune cycles, mirroring neuromodulatory control\nof consolidation.\nTogether these mechanisms mark a shift from passive decay toward context-sensitive, learned forgetting that\nprotects critical memories while continuously liberating capacity for new experiences.\nCapacity Optimization. Capacity optimization addresses how memory-augmented Transformers expand\nstorage and retrieval ability without linearly inflating computation or parameters. Current work converges\non three complementary tactics\u2014compression, hierarchy, and sparsity\u2014to keep memory growth compatible\nwith practical hardware budgets.\nCompressive techniques shrink inactive activations or knowledge chunks before eviction. The Compressive\nTransformer (Rae et al., 2019) auto-encodes aged hidden states into coarse vectors, doubling usable context\nwhile holding FLOPs steady and matching baseline perplexity on WikiText-103 . At the knowledge level,\nEMAT (Wu et al., 2022b) and MATTER (Lee et al., 2024) map millions of QA pairs or mixed documents\nto short neural codes; maximum-inner-product search then delivers sub-millisecond retrieval without adding\ntrainable weights.\nHierarchical organization spreads capacity across tiers with different granularity. MemLong (Liu et al., 2024)\nchunks sequences and prunes rarely accessed blocks, maintaining 80 K-token windows on a single GPU. LM2\n(Kang et al., 2025b) builds tree-indexed memories that let local detail and global context be fetched at\nequal cost, sustaining reasoning over 128 K tokens. HMT (He et al., 2024a) stacks sensory, short-term, and\nlong-term buffers, matching large long-context models while using \u22482% of their parameters.\nSparse look-ups push size further by reducing per-query work. Memory Layers at Scale shards (Berges et al.,\n2024) product-key tables across GPUs, supporting billion-entry memories with sub-linear compute and intact\nend-to-end gradients . Dynamic Memory Compression Nawrot et al. (2024) learns head- and layer-specific\nKV sharing, cutting inference memory up to 4 \u00d7with negligible accuracy drop , while MLKV Zuhri et al.\n(2024) shares KV heads across layers to trim cache by up to 6 \u00d7at similar quality.\nTogether, these advances show that intelligent compression, hierarchical buffering, and sparse retrieval make\nlarge-capacity memory feasible, allowing even modest-sized Transformers to reason over book-length context\nor web-scale knowledge without prohibitive cost.\nSelf-Management and Adaptation. After compression and hierarchical layout tame raw capacity, the\nnexthurdleisdecidinghowthatcapacityisusedinrealtime. Recentmodelstreatmemoryasanautonomous\nresource that can be allocated, specialised, or pruned during inference, bringing Transformers closer to the\nselective plasticity of biological cognition.\nTransformer-Squared (Sun et al., 2025) routes activations through a pool of expert vectors selected on-the-\nfly, letting the model enlarge functional capacity without weight updates while preserving high accuracy on\nunseen procedural tasks. Titans (Behrouz et al., 2024) adds a neuromodulatory gate: only tokens whose\nKL-surprise clears a learned threshold are written, and low-surprise traces decay, reducing interference while\nthe long-term store grows during deployment. ATLAS (Behrouz et al., 2025) generalises this to sliding\nwindows; its Omega rule re-weights entire spans, down-scoring stale patterns and allowing super-linear\nmemory growth without gradient descent. NAMMs (Cetin et al., 2025) evolve layer-wise retention masks\nfrom attention statistics, trimming key\u2013value caches by up to 80% yet improving long-context benchmarks\nthrough zero-shot transfer across modalities.\n18\n\n--- Page 19 ---\nInterference control is handled by orthogonal or gated rewrites. ARMT (Rodkin et al., 2024) projects\nnew vectors onto an orthogonal subspace before insertion, preventing outdated attractors and keeping O(1)\nretrieval stable over tens of millions of tokens. MemReasoner (Ko et al., 2024) iteratively re-reads and\nupdates a temporal store with bidirectional GRUs until the representation converges, preventing early facts\nfrom being overwritten and boosting multi-hop question answering on 128 k-token documents. RA-DT\n(Schmied et al., 2024) links episodic memory to a reinforcement-learning critic, retaining only high-error\ntrajectories and lifting multi-task sample efficiency while bounding memory size. MemBART (Wu & Yu,\n2022) isolates dialogue context from world knowledge through dual attention streams and residual gates, and\nSchr\u00f6dinger\u2019s Memory (Wang & Li, 2024) stores traces in a latent \u201csuperposition\u201d that surface only when\ncued, lowering hallucination rates in factual probing.\nCollectively, these systems replace static buffers with self-monitoring stores that learn what to remember,\nwhere to place it, and when to forget. By coupling dynamic allocation with interference-aware rewriting,\nthey extend the compression and hierarchy tools of capacity optimisation into a full feedback loop, allowing\nTransformers to balance stability and plasticity throughout their lifetime.\n5 Discussion, Challenges, and Future Directions\nMemory-augmented Transformers have progressed from simple context extensions to sophisticated cognitive\narchitectures, narrowing the gap between learning and memory. Our taxonomic analysis in Table 2 shows\na rapid shift from static pattern recognition to adaptive, experience-driven intelligence. From the earliest\nsystems in 2019 to today\u2019s production-ready designs, development has converged toward hybrid storage,\nadaptive dynamics, and intelligent forgetting, while also exposing persistent challenges in scaling, evalua-\ntion, and integration. This chapter distills insights from that evolution, highlights constraints that limit\ncurrent models, and outlines research directions to bridge artificial and biological memory systems\u2014offering\na roadmap toward architectures that not only extend computational capacity but also support genuine\nartificial cognition.\n5.1 Overview and Synthesis\nEvolutionary trajectory and convergence\n\u2022Foundation (2019\u20132021): Early systems established explicit memory management beyond standard\nattention via state-based recurrence and compression, demonstrating that long-range modeling ben-\nefits from persistent activations and hierarchical reduction (e.g., Transformer-XL Dai et al. (2019);\nCompressive Transformer Rae et al. (2019)).\n\u2022Expansion (2022\u20132024): Retrieval-augmented modeling scaled access from thousands to billions of\nentries using kNN/MIPS indexing and chunked cross-attention (e.g., Memorizing Transformer Wu\net al. (2022a); RETRO Borgeaud et al. (2022); EMAT Wu et al. (2022b); Memory Layers at Scale\nBerges et al. (2024)), while architectures diversified to associative, hierarchical, and graph-based\norganization (e.g., AiT Sun et al. (2023), MemGPT Packer et al. (2023), MemWalker Chen et al.\n(2023), HippoRAG Guti\u00e9rrez et al. (2024), CDMem Gao et al. (2025)). Surprise-gated updates\nemerged as a biologically motivated write policy (e.g., Titans Behrouz et al. (2024)), complementing\nselective reset/decay and LRU strategies for stability under growth.\n\u2022Maturation (2025): Production-oriented designs emphasized hybrid storage, test-time adaptation,\nand specialized access, including expert routing and compression-based interfaces (e.g., zip2zip Geng\net al. (2025)), operational memory OS abstractions (e.g., MemoryOS Kang et al. (2025a)), and\nhierarchical controllers for reasoning (e.g., HRM Wang et al. (2025a)), reflecting a shift from static\npattern recognition to adaptive, experience-driven intelligence.\nArchitecture: hybrid dominance. Parameter-encoded memory offers immediate access but risks catas-\ntrophic interference when updated; state-based memory supports rapid adaptation but is capacity-limited;\nexternal stores scale but add retrieval/consistency overhead. Hybrid designs increasingly combine these\n19\n\n--- Page 20 ---\nTable 2: Comprehensive feature matrix for memory-augmented Transformer models: Evolution from 2019-\n2025\nYear Model Architecture Generality Memory Dynamics Management\nStorage Class\nIntegration Method\nBackbone Compatibility\nInput Modality\nMemory Span\nWrite Trigger\nPlasticity\nMemory Scope\nRetrieval Mechanism\nForgetting Mechanism\n2019 Transformer-XL (Dai et al., 2019) SWrp \u00d7TS Stc FLyr Attn FIFO\nCompressive Transformer Rae et al. (2019) S Wrp \u00d7T M Stc F Lyr Attn Dec+FIFO\n2020 SAM (Le et al., 2020) E Plg \u2713MA Pol TTGbl Outer Rst\nMemformer Wu et al. (2020) H(SE) Plg \u2713M L G TT Gbl Attn Dec\n2021 ABC Peng et al. (2021) E Plg \u2713TS Pol FGbl Pol \u2014\nWorkMATe (Kruijne et al., 2021) S Plg \u2713T S Pol TT Lyr Attn \u2014\n2022 EMAT (Wu et al., 2022b) E Plg \u2713TL Stc FGbl MIPS \u2014\nRETRO (Borgeaud et al., 2022) E Plg \u2713T L Stc F Gbl kNN \u2014\nDSI (Tay et al., 2022) P Bsp \u00d7TL Stc FLyr Attn \u2014\nMemorizing Transf. Wu et al. (2022a) E Plg \u2713T L Stc F Gbl kNN \u2014\nMemBART (Wu & Yu, 2022) S Plg \u00d7TL G TTHrch Dual Rst\n2023 LongMem (Wang et al., 2023) E Plg \u2713T L G TT Gbl Attn Prn\nMemGPT Packer et al. (2023) E Plg \u2713TM G TTHrch kNN LRU\nThink-in-Memory (Liu et al., 2023) E Plg \u2713T L G TT Gbl Trip Dec\nAdaTape (Xue et al., 2023) H(PS) Plg \u2713TS Pol FLyr Tape \u2014\nMemWalker (Chen et al., 2023) E Bsp \u2713T M Pol TT Hrch Tree Rst\nAiT (Sun et al., 2023) H Plg \u2713MA Pol TTGbl Assoc Dec\nMeMOTR Gao & Wang (2023) H(SE) Bsp \u00d7M L Stc TT Gbl Attn Dec\n2024 MemoryBank Zhong et al. (2024a) E Plg \u2713TL G TTHrch kNN Dec\nTransformerFAM Hwang et al. (2024) S Plg \u2713T S Stc F Lyr Attn \u2014\nHMT He et al. (2024a) H(SE) Plg \u2713TL Stc FHrch Attn Dec\nMemoryLLM (Wang et al., 2024b) H(SE) Plg \u2713T L Stc TT Gbl Attn Dec\nHippoRAG Guti\u00e9rrez et al. (2024) E Plg \u2713TL Stc TTGbl Graph Rst\nMATTER Lee et al. (2024) H(PE) Wrp \u2713T M Stc F Gbl MIPS \u2014\nMemory3 Yang et al. (2024) H Plg \u2713TMStc+Pol FGbl kNN \u2014\nARMT (Rodkin et al., 2024) E Plg \u2713T A Pol TT Hrch Assoc Cyc\nMemLong Liu et al. (2024) E Plg \u2713TL G TTGbl kNN Prn\nSchr\u00f6dinger\u2019s Memory Wang & Li (2024) P Bsp \u00d7T L Stc F Gbl Attn \u2014\nMemReasoner Ko et al. (2024) E Plg \u2713TL Stc TTGbl Attn Rst\nEM-LLM (Fountas et al., 2024) E Plg \u2713T L Sur TT Gbl Seg+kNN Dec\nRA-DT (Schmied et al., 2024) E Plg \u2713ML Sur TTGbl Pol Sel\nMemory Layers at Scale (Berges et al., 2024) P Bsp \u00d7T L Stc F Gbl PK-MIPS \u2014\nTitans Behrouz et al. (2024) H(PE) Wrp \u2713TM Sur TTGbl Attn Dec\nTransformer-Squared (Sun et al., 2025) P Wrp \u2713T L G TT Lyr Attn \u2014\n2025 LM2 Kang et al. (2025b) H(SE) Wrp \u2713TM G TTHrch Attn Dec\nNAMMs (Cetin et al., 2025) H(PSE) Plg \u2713M M Pol TT Hrch Pol Prn\nR3mem (Wang et al., 2025b) H(PE) Plg \u2713TM G TTGbl Comp Sel\nRMoE (Qiu et al., 2024) S Plg \u2713T S G TT Lyr Hier Sel\nMemory-R+ (Le et al., 2025) E Plg \u2713TL Sur TTGbl kNN Sel\nMem0 Chhikara et al. (2025) E Plg \u2713T L G TT Hrch kNN+Graph LRU\nCDMem (Gao et al., 2025) E Plg \u2713TM Pol TTHrch Graph Rst\nATLAS Behrouz et al. (2025) H(PE) Wrp \u2713T M Sur TT Gbl Attn Dec\nMemoryOS (Kang et al., 2025a) E Plg \u2713TM G TTHrch Seg LRU\nzip2zip (Geng et al., 2025) P Wrp \u2713T S Pol TT Gbl Comp \u2014\nPeripheral Memory (Zhai et al., 2025) P Plg \u2713TS G TTGbl Attn \u2014\nMALT Diffusion (Yu et al., 2025) S Plg \u2713M L Stc F Lyr RecAtt \u2014\nA-MEM (Xu et al., 2025) E Plg \u2713ML G TTGbl kNN Evol\nHRM (Wang et al., 2025a) S Bsp \u00d7T M Pol TT Hrch Hier Rst\nLegend:\nArchitecture:\nStorage Class: P = Parameter-encoded, S =\nState-based, E = External store, H = Hybrid\n, H(PS) = Parameter+State, H(PE) = Parame-\nter+External, H(SE) = State+External, H(PSE)\n= All three\nIntegration Method: Plg = Plug-in, Wrp = Wrap-\nper/adaptor, Bsp = Bespoke redesign\nBackbone Compatibility: \u2713= Universal, \u00d7=\nArchitecture-specific\nGenerality:\nInput Modality: T = Text-only, M = Multi-modalMemory Span: S = Short-term, L = Long-term, M\n= Multi-scale, A = Associative\nMemory Dynamics:\nWrite Trigger: Stc = Static, Sur = Surprise-gated,\nPol = Policy-learned, G = Gated\nPlasticity: F = Fixed after training, TT = Test-\ntime adaptable\nMemory Scope: Lyr = Layer-local, Gbl = Global,\nHrch = Hierarchical\nManagement:\nRetrieval Mechanism: Attn = Attention-based,\nkNN = k-Nearest neighbor, Assoc = Associa-\ntive, Graph = Graph-based, MIPS = Max in-\nner product search, PK-MIPS = Product-keyMIPS, Pol = Policy-driven, Seg = Segmenta-\ntion, Outer = Outer-product, Trip = Triplet-\nbased, Expert = Expert-routing, Tree = Tree-\nbased, Tape = Tape-based, RecAtt = Recur-\nrent attention, Dual = Dual-stream, Comp =\nCompression-based, Hier = Hierarchical\nForgetting Mechanism: FIFO = First-in-first-out,\nDec = Decay, Prn = Pruning, Cyc = Cycle-\nbased, Rst = Reset, LRU = Least-recently-used,\nSel = Selective, Evol = Evolutionary\nTimeline: 2019 2020 2021\n2022 2023 2024 2025\nNote:\u2014 = Not reported\n20\n\n--- Page 21 ---\nmodalities to balance latency, scalability, and plasticity through division of labor and policy-driven coordi-\nnation.\nMemory dynamics: from rules to policies. Write operations progressed from static schedules to\nsurprise-gated consolidation and learned policies, mitigating stability\u2013plasticity trade-offs by updating on\nprediction errors and adapting allocation/eviction to task demands. Test-time plasticity became the default,\nenabling personalization and continual adaptation in deployment.\nRetrieval and forgetting: specialization matters. Access evolved beyond attention and pure sim-\nilarity toward structure-aware methods: graph navigation for relational queries, associative retrieval for\ncontent-addressable access, and hierarchical/expert routing for specialization and efficiency. Forgetting\nmoved from FIFO/decay to LRU, selective, cycle-based, and evolutionary strategies, showing that intel-\nligent erasure\u2014aligned with utility and hierarchy\u2014is as consequential as storage and retrieval for sustained\nperformance under growth.\n5.2 Challenges\nDespite remarkable progress toward cognitive memory architectures, our comprehensive analysis reveals\nfundamental challenges that continue to constrain practical deployment and theoretical understanding of\nmemory-augmented systems.\nScalabilityandRetrievalBottlenecks. Despitesignificantarchitecturalinnovations,memory-augmented\nsystemsfacefundamentalscalabilityconstraintsthatlimitpracticaldeploymentatscale. Currentapproaches\ndemonstrate distinct trade-offs between computational efficiency, retrieval accuracy, and resource require-\nments.\nRetrieval mechanisms exhibit characteristic scaling limitations. Approximate similarity search methods,\nwhilecomputationallyefficient, sufferfromaccuracydegradationasmemorysizeincreases(Wuetal.,2022b).\nProduct-key decomposition approaches successfully reduce lookup complexity from linear to sub-linear scal-\ning(Bergesetal.,2024),yetencounterparameteroverheadthatconstrainsexpansiontobillion-entrysystems.\nGraph-based retrieval methods enable sophisticated multi-hop reasoning but face exponential complexity\ngrowth with increasing graph density and traversal depth.\nCompression-based solutions present complementary challenges. While techniques like adaptive tokenization\ncan achieve substantial sequence reduction, they introduce inference-time computational overhead that may\noffset retrieval gains. Hierarchical memory organization similarly requires careful balance between compres-\nsion efficiency and information fidelity.\nInfrastructure considerations increasingly limit deployment viability. Memory-augmented architectures im-\npose substantial storage bandwidth requirements, introduce novel security vulnerabilities through persistent\nexternal memory, and exhibit non-linear energy scaling patterns. Distributed implementations face addi-\ntional consistency and latency challenges that can negate theoretical performance advantages. Cross-modal\nsystems compound these issues by requiring unified similarity metrics that may inadequately represent het-\nerogeneous data types, leading to systematic retrieval degradation across modalities.\nMemory Interference and Coordination. Memory systems face fundamental coordination challenges\nbeyond scalability, particularly in multi-task scenarios. The stability-plasticity dilemma manifests distinctly\nacross architectures: while current literature confirms surprise-gated systems excel at novelty detection,\nevidence for their specific struggles with gradual knowledge drift remains limited. Policy-learned approaches\nshow strong task adaptation capabilities but face inherent overfitting risks during continual learning.\nMemory interference emerges as a critical bottleneck when similar contexts trigger conflicting informa-\ntion retrieval. External memory systems suffer from catastrophic collisions during concurrent access, while\nparameter-encoded approaches experience gradient interference during continual updates. Hybrid archi-\ntectures attempt to mitigate these issues through functional partitioning, yet optimal allocation strategies\nremain highly domain-dependent.\nForgetting policies introduce additional coordination complexities. A-MEM\u2019s Xu et al. (2025) evolutionary\napproach demonstrates promise for utility-based memory curation but requires careful tuning to avoid inad-\n21\n\n--- Page 22 ---\nvertent erasure of rare but valuable information. Systems employing LRU and selective forgetting strategies\nperform effectively in structured environments but struggle under non-stationary conditions where relevance\npatterns shift unpredictably.\nEvaluation and Standardization Gaps. The field suffers from fragmented evaluation methodologies that\nprevent systematic cross-architecture comparisons. Current benchmarks exhibit dramatic variation in mem-\noryrequirements, taskcomplexity, andevaluationmetrics, makingitdifficulttoassessfundamentaltrade-offs\nbetween different memory strategies. Critical deficiencies include inconsistent context length protocols, di-\nvergent benchmark emphases on retrieval versus reasoning capabilities, and absent robustness evaluations\nfor adversarial scenarios, memory corruption, and distribution shift. Most significantly, existing evaluations\nrarely assess long-term adaptation, memory utilization efficiency, or interference mitigation\u2014precisely the\ncapabilities that distinguish sophisticated memory systems from basic approaches.\n5.3 Future Directions\nThe evolution of memory-augmented Transformers toward truly cognitive architectures requires coordinated\nadvances across multiple research frontiers. The following directions represent the most promising paths for\nachieving human-like memory capabilities while addressing current technical limitations.\nToward Cognitive Flexibility and Lifelong Learning. Emerging paradigms in memory-augmented\ntransformers focus on building systems that can dynamically store, retrieve, and update knowledge in ways\nthat reflect the adaptability of biological memory. Neuroscientific insights from Dijksterhuis et al. (2024)\nhighlight the value of memory consolidation, revealing how concept cells in the human hippocampus reac-\ntivate when pronouns reference specific nouns, seamlessly linking new linguistic input to stored concepts-a\nmechanism comparable to integrating episodic memories into an LLM\u2019s parametric memory to bypass capac-\nity constraints and achieve lasting retention. Complementing this, the position paper by Pink et al. (2025)\nargues that episodic memory is a vital missing component for long-term LLM agents, proposing a frame-\nwork with five essential properties to foster adaptive behavior and outlining a research roadmap to embed\nthese capabilities. A pivotal trend in this direction is the decoupling of computation from storage, enabling\nmodels to tap into external or hybrid memory banks for real-time, current information without the need\nfor retraining. Such architectures facilitate personalized, context-responsive outputs in evolving environ-\nments. Furthermore, innovations like test-time training and memory-driven optimization empower models\nto learn and adapt during deployment, bolstered by selective forgetting and zero-shot transfer mechanisms\nthat enhance generalization. The integration of multimodal memory and collaborative networks also holds\npromise for deeper reasoning and shared learning among agents. To sustain these advancements, progress\nin hierarchical storage, memory compression, and hardware-aware design is driving scalable, energy-efficient\ndeployment across varied platforms.\nToward Human-Like Cognition: The Role of Memory in Intelligent Agents. As intelligent agents\nevolve toward more human-like reasoning and autonomy, the integration of sophisticated memory systems\nbecomes a central design challenge. Unlike conventional LLMs that operate in a stateless fashion, truly\ninteractive agents must preserve context, interpret ongoing events, and adapt their behavior across time\n(Liang et al., 2024; Yi et al., 2025). Drawing from cognitive science, recent agent architectures incorporate\nshort-term memory for maintaining dialogue context, working memory for in-the-moment reasoning, and\nlong-term memory for accumulating knowledge and past experiences (Li et al., 2024). Vector databases have\nemergedasapopularsolutionforimplementinglong-termmemory, enablingfast, similarity-basedretrievalof\nepisodic and procedural knowledge (Hatalis et al., 2023). However, realizing robust memory-driven behavior\nintroduces significant difficulties. Agents often fail to separate memory types, leading to conflicts between\nepisodic and semantic recall, and may repeatedly attempt failed subtasks without effective use of episodic\nfeedback (Wang et al., 2024c). As memory grows over time, retrieval speed and storage cost become critical\nconcerns, especially when managing large volumes of data. Static or manually defined metadata can limit\nretrieval quality, pointing to a need for agents to learn metadata attributes dynamically to support smarter\ndecision-making (Sarch et al., 2023). Moreover, integrating long-term memory with external knowledge bases\nlike ontologies or knowledge graphs could enhance contextual grounding and reasoning (Wang et al., 2024c).\nAddressing these issues is essential to building agents capable of flexible, adaptive cognition that mirrors the\nstructure and function of human memory systems.\n22\n\n--- Page 23 ---\nFuture Architectures and Ethical Considerations. Test-time training, memory-driven optimization,\nand zero-shot transfer learning allow models to adapt during deployment, offering the promise of lifelong\nlearning. Multimodal memory systems and collaborative agent networks open new paths for collective\nintelligence, deeper reasoning, and shared learning across environments. Yet, these capabilities also introduce\nethical and societal considerations. As memory-augmented Transformers are adopted in sensitive domains\nlike healthcare, education, and personalized services, ensuring transparency, privacy, and user control over\nmemory becomes vital. Techniques for explainable memory operations, data auditing, and bias mitigation\nwill be critical to build trust and prevent misuse.\nIn summary, the future of memory-augmented Transformers lies in bridging engineering efficiency with\ncognitive flexibility. By combining continual learning, dynamic memory adaptation, and biologically inspired\ndesign, alongside ethical safeguards, these systems have the potential to transform AI from static pattern\nrecognizers into adaptive, intelligent agents.\nReferences\nChristophAnackerandRen\u00e9Hen. Adulthippocampalneurogenesisandcognitiveflexibility\u2014linkingmemory\nand mood. Nature Reviews Neuroscience , 18(6):335\u2013346, 2017.\nBernard J Baars, Natalie Geld, and Robert Kozma. Global workspace theory (gwt) and prefrontal cortex:\nRecent developments. Frontiers in psychology , 12:749868, 2021.\nAlan Baddeley. Working memory: looking back and looking forward. Nature reviews neuroscience , 4(10):\n829\u2013839, 2003.\nMartin LLR Barry and Wulfram Gerstner. Fast adaptation to rule switching using neuronal surprise. PLoS\ncomputational biology , 20(2):e1011839, 2024.\nAmjad H Bazzari and H Rheinallt Parri. Neuromodulators and long-term synaptic plasticity in learning and\nmemory: A steered-glutamatergic perspective. Brain sciences , 9(11):300, 2019.\nAli Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time. arXiv preprint\narXiv:2501.00663 , 2024.\nAli Behrouz, Zeman Li, Praneeth Kacham, Majid Daliri, Yuan Deng, Peilin Zhong, Meisam Razaviyayn,\nand Vahab Mirrokni. Atlas: Learning to optimally memorize the context at test time. arXiv preprint\narXiv:2505.23735 , 2025.\nIz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150 , 2020.\nVincent-Pierre Berges, Barlas O\u011fuz, Daniel Haziza, Wen-tau Yih, Luke Zettlemoyer, and Gargi Ghosh.\nMemory layers at scale. arXiv preprint arXiv:2412.09764 , 2024.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving\nlanguage models by retrieving from trillions of tokens. In International conference on machine learning ,\npp. 2206\u20132240. PMLR, 2022.\nEduardo Camina and Francisco G\u00fcell. The neuroanatomical, neurophysiological and psychological basis of\nmemory: Current models and their origins. Frontiers in pharmacology , 8:438, 2017.\nEdoardo Cetin, Qi Sun, Tianyu Zhao, and Yujin Tang. An evolved universal transformer memory. In The\nThirteenth International Conference on Learning Representations , 2025.\nHoward Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz. Walking down the memory maze:\nBeyond context limit through interactive reading. arXiv preprint arXiv:2310.05029 , 2023.\nPrateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. Mem0: Building\nproduction-ready ai agents with scalable long-term memory. arXiv preprint arXiv:2504.19413 , 2025.\n23\n\n--- Page 24 ---\nMarvin M Chun and Nicholas B Turk-Browne. Interactions between attention and memory. Current opinion\nin neurobiology , 17(2):177\u2013184, 2007.\nNicholas G Cicero, Elizabeth Riley, Khena M Swallow, Eve De Rosa, and Adam Anderson. Attention-\ndependent coupling with forebrain and brainstem neuromodulatory nuclei differs across the lifespan. Gero-\nScience, pp. 1\u201320, 2025.\nNelson Cowan. What are the differences between long-term, short-term, and working memory? Progress in\nbrain research , 169:323\u2013338, 2008.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdi-\nnov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint\narXiv:1901.02860 , 2019.\nStanislas Dehaene, Jean-Pierre Changeux, and Lionel Naccache. The global neuronal workspace model of\nconscious access: from neuronal architectures to clinical applications. Characterizing consciousness: From\ncognition to the clinic? , pp. 55\u201384, 2011.\nDoris E Dijksterhuis, Matthew W Self, Jessy K Possel, Judith C Peters, ECW van Straaten, Sander Idema,\nJohannes C Baaijen, Sandra MA van der Salm, Erik J Aarnoutse, Nicole CE van Klink, et al. Pronouns\nreactivateconceptualrepresentationsinhumanhippocampalneurons. Science, 385(6716):1478\u20131484, 2024.\nFlorin Dolcos, Kevin S LaBar, and Roberto Cabeza. Interaction between the amygdala and the medial\ntemporal lobe memory system predicts better memory for emotional events. Neuron, 42(5):855\u2013863, 2004.\nYiming Du, Wenyu Huang, Danna Zheng, Zhaowei Wang, Sebastien Montella, Mirella Lapata, Kam-Fai\nWong, and Jeff Z Pan. Rethinking memory in ai: Taxonomy, operations, topics, and future directions.\narXiv preprint arXiv:2505.00675 , 2025.\nSanchayan Dutta and Suvrit Sra. Memory-augmented transformers can implement linear first-order opti-\nmization methods. arXiv preprint arXiv:2410.07263 , 2024.\nMarco Federici, Davide Belli, Mart Van Baalen, Amir Jalalirad, Andrii Skliar, Bence Major, Markus Nagel,\nand Paul Whatmough. Efficient llm inference using dynamic input pruning and cache-aware masking.\narXiv preprint arXiv:2412.01380 , 2024.\nZafeirios Fountas, Martin A Benfeghoul, Adnan Oomerjee, Fenia Christopoulou, Gerasimos Lampouras,\nHaitham Bou-Ammar, and Jun Wang. Human-like episodic memory for infinite context llms. arXiv\npreprint arXiv:2407.09450 , 2024.\nDarya Frank, Alex Kafkas, and Daniela Montaldi. Experiencing surprise: The temporal dynamics of its\nimpact on memory. Journal of Neuroscience , 42(33):6435\u20136444, 2022.\nPengyuGao,JinmingZhao,XinyueChen,andLongYilin. Anefficientcontext-dependentmemoryframework\nfor llm-centric agents. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies (Volume 3: Industry Track) ,\npp. 1055\u20131069, 2025.\nRuopeng Gao and Limin Wang. Memotr: Long-term memory-augmented transformer for multi-object track-\ning. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 9901\u20139910,\n2023.\nAdamGazzaleyandAnnaCNobre. Top-downmodulation: bridgingselectiveattentionandworkingmemory.\nTrends in cognitive sciences , 16(2):129\u2013135, 2012.\nSaibo Geng, Nathan Ranchin, Maxime Peyrard, Chris Wendler, Michael Gastpar, Robert West, et al.\nzip2zip: Inference-time adaptive vocabularies for language models via token compression. arXiv preprint\narXiv:2506.01084 , 2025.\n24\n\n--- Page 25 ---\nPaul E Gilbert and Andrea M Brushfield. The role of the ca3 hippocampal subregion in spatial memory: a\nprocess oriented behavioral assessment. Progress in Neuro-Psychopharmacology and Biological Psychiatry ,\n33(5):774\u2013781, 2009.\nAlex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401 ,\n2014.\nAlex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwi\u0144ska,\nSergio G\u00f3mez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid computing\nusing a neural network with dynamic external memory. Nature, 538(7626):471\u2013476, 2016.\nBernal Jim\u00e9nez Guti\u00e9rrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. Hipporag: Neurobiologically\ninspired long-term memory for large language models. In The Thirty-eighth Annual Conference on Neural\nInformation Processing Systems , 2024.\nUri Hasson, Janice Chen, and Christopher J Honey. Hierarchical process memory: memory as an integral\ncomponent of information processing. Trends in cognitive sciences , 19(6):304\u2013313, 2015.\nKostas Hatalis, Despina Christou, Joshua Myers, Steven Jones, Keith Lambert, Adam Amos-Binks, Zohreh\nDannenhauer, and Dustin Dannenhauer. Memory matters: The need to improve long-term memory in\nllm-agents. In Proceedings of the AAAI Symposium Series , volume 2, pp. 277\u2013280, 2023.\nZifanHe, YingqiCao, ZongyueQin, NehaPrakriya, YizhouSun, andJasonCong. Hmt: Hierarchicalmemory\ntransformer for efficient long context language processing. arXiv preprint arXiv:2405.06067 , 2024a.\nZihong He, Weizhe Lin, Hao Zheng, Fan Zhang, Matt W Jones, Laurence Aitchison, Xuhai Xu, Miao Liu,\nPer Ola Kristensson, and Junxiao Shen. Human-inspired perspectives: A survey on ai long-term memory.\narXiv preprint arXiv:2411.00489 , 2024b.\nCameron Higgins, Yunzhe Liu, Diego Vidaurre, Zeb Kurth-Nelson, Ray Dolan, Timothy Behrens, and Mark\nWoolrich. Replay bursts in humans coincide with activation of the default mode and parietal alpha\nnetworks. Neuron, 109(5):882\u2013893, 2021.\nDongseong Hwang, Weiran Wang, Zhuoyuan Huo, Khe Chai Sim, and Pedro Moreno Mengibar. Trans-\nformerfam: Feedback attention is working memory. arXiv preprint arXiv:2404.09173 , 2024.\nJiazheng Kang, Mingming Ji, Zhe Zhao, and Ting Bai. Memory os of ai agent. arXiv preprint\narXiv:2506.06326 , 2025a.\nJikun Kang, Wenqi Wu, Filippos Christianos, Alex J Chan, Fraser Greenlee, George Thomas, Marvin\nPurtorab, and Andy Toulis. Lm2: Large memory models. arXiv preprint arXiv:2502.06049 , 2025b.\nLouis Kang and Taro Toyoizumi. Distinguishing examples while building concepts in hippocampal and\nartificial networks. Nature Communications , 15(1):647, 2024.\nJens G Klinzing, Niels Niethard, and Jan Born. Mechanisms of systems memory consolidation during sleep.\nNature neuroscience , 22(10):1598\u20131610, 2019.\nChing-Yun Ko, Sihui Dai, Payel Das, Georgios Kollias, Subhajit Chaudhury, and Aurelie Lozano. Memrea-\nsoner: A memory-augmented llm architecture for multi-hop reasoning. In The First Workshop on System-2\nReasoning at Scale, NeurIPS\u201924 , 2024.\nWouter Kruijne, Sander M Bohte, Pieter R Roelfsema, and Christian NL Olivers. Flexible working memory\nthrough selective gating and attentional tagging. Neural Computation , 33(1):1\u201340, 2021.\nAbhilasha A Kumar. Semantic memory: A review of methods, models, and current challenges. Psychonomic\nbulletin & review , 28(1):40\u201380, 2021.\nHung Le, Truyen Tran, and Svetha Venkatesh. Self-attentive associative memory. In International conference\non machine learning , pp. 5682\u20135691. PMLR, 2020.\n25\n\n--- Page 26 ---\nHung Le, Dai Do, Dung Nguyen, and Svetha Venkatesh. Reasoning under 1 billion: Memory-augmented\nreinforcement learning for large language models. arXiv preprint arXiv:2504.02273 , 2025.\nDongkyu Lee, Chandana Satya Prakash, Jack FitzGerald, and Jens Lehmann. Matter: Memory-augmented\ntransformer using heterogeneous knowledge sources. arXiv preprint arXiv:2406.04670 , 2024.\nJonathan LC Lee, Karim Nader, and Daniela Schiller. An update on memory reconsolidation updating.\nTrends in cognitive sciences , 21(7):531\u2013545, 2017.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich\nK\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge-\nintensive nlp tasks. Advances in neural information processing systems , 33:9459\u20139474, 2020.\nZeyuan Li, Yangfan He, Lewei He, Jianhui Wang, Tianyu Shi, Bin Lei, Yuchen Li, and Qiuwu Chen. Falcon:\nFeedback-driven adaptive long/short-term memory reinforced coding optimization system. arXiv preprint\narXiv:2410.21349 , 2024.\nXuechen Liang, Yangfan He, Yinghui Xia, Xinyuan Song, Jianhui Wang, Meiling Tao, Li Sun, Xinhang\nYuan, Jiayi Su, Keqin Li, et al. Self-evolving agents with reflective and memory-augmented abilities.\narXiv preprint arXiv:2409.00872 , 2024.\nBang Liu, Xinfeng Li, Jiayi Zhang, Jinlin Wang, Tanjin He, Sirui Hong, Hongzhang Liu, Shaokun Zhang,\nKaitao Song, Kunlun Zhu, et al. Advances and challenges in foundation agents: From brain-inspired\nintelligence to evolutionary, collaborative, and safe systems. arXiv preprint arXiv:2504.01990 , 2025.\nLei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang.\nThink-in-memory: Recalling and post-thinking enable llms with long-term memory. arXiv preprint\narXiv:2311.08719 , 2023.\nWeijie Liu, Zecheng Tang, Juntao Li, Kehai Chen, and Min Zhang. Memlong: Memory-augmented retrieval\nfor long text modeling. arXiv preprint arXiv:2408.16967 , 2024.\nYunzhe Liu, Marcelo G Mattar, Timothy EJ Behrens, Nathaniel D Daw, and Raymond J Dolan. Experience\nreplay is associated with efficient nonlocal learning. Science, 372(6544):eabf1357, 2021.\nWenhan Luo, Di Yun, Yi Hu, Miaomiao Tian, Jiajun Yang, Yifan Xu, Yong Tang, Yang Zhan, Hong\nXie, and Ji-Song Guan. Acquiring new memories in neocortex of hippocampal-lesioned mice. Nature\ncommunications , 13(1):1601, 2022.\nGuixiang Ma, Vy A Vo, Theodore L Willke, and Nesreen K Ahmed. Memory-augmented graph neural\nnetworks: A brain-inspired review. IEEE Transactions on Artificial Intelligence , 5(5):2011\u20132025, 2023.\nMartial Mermillod, Aur\u00e9lia Bugaiska, and Patrick Bonin. The stability-plasticity dilemma: Investigating\nthe continuum from catastrophic forgetting to age-limited learning effects, 2013.\nGeorge A Miller. The magical number seven, plus or minus two: Some limits on our capacity for processing\ninformation. Psychological review , 63(2):81, 1956.\nAliModarressi,AbdullatifK\u00f6ksal,AyyoobImani,MohsenFayyaz,andHinrichSch\u00fctze. Memllm: Finetuning\nllms to use an explicit read-write memory. arXiv preprint arXiv:2404.11672 , 2024.\nMorris Moscovitch, Roberto Cabeza, Gordon Winocur, and Lynn Nadel. Episodic memory and beyond: the\nhippocampus and neocortex in transformation. Annual review of psychology , 67(1):105\u2013134, 2016.\nSwaleha Mujawar, Jaideep Patil, Bhushan Chaudhari, and Daniel Saldanha. Memory: Neurobiological\nmechanisms and assessment. Industrial psychiatry journal , 30(Suppl 1):S311\u2013S314, 2021.\nThomas Nail. Most brain activity is \u2018background noise\u2019\u2014and that\u2019s upending our understanding of con-\nsciousness, 2021.\n26\n\n--- Page 27 ---\nPiotr Nawrot, Adrian \u0141a\u0144cucki, Marcin Chochowski, David Tarjan, and Edoardo M Ponti. Dynamic memory\ncompression: Retrofitting llms for accelerated inference. arXiv preprint arXiv:2403.09636 , 2024.\nErika Nyhus and Tim Curran. Functional role of gamma and theta oscillations in episodic memory. Neuro-\nscience & Biobehavioral Reviews , 34(7):1023\u20131035, 2010.\nCharlesPacker, VivianFang, Shishir_GPatil, KevinLin, SarahWooders, andJoseph_EGonzalez. Memgpt:\nTowards llms as operating systems. 2023.\nHao Peng, Jungo Kasai, Nikolaos Pappas, Dani Yogatama, Zhaofeng Wu, Lingpeng Kong, Roy Schwartz,\nand Noah A Smith. Abc: Attention with bounded-memory control. arXiv preprint arXiv:2110.02488 ,\n2021.\nMathis Pink, Qinyuan Wu, Vy Ai Vo, Javier Turek, Jianing Mu, Alexander Huth, and Mariya Toneva.\nPosition: Episodic memory is the missing piece for long-term llm agents. arXiv preprint arXiv:2502.06975 ,\n2025.\nOfir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input\nlength extrapolation. arXiv preprint arXiv:2108.12409 , 2021.\nLuke Y Prince, Travis J Bacon, Cezar M Tigaret, and Jack R Mellor. Neuromodulation of the feedforward\ndentate gyrus-ca3 microcircuit. Frontiers in synaptic neuroscience , 8:32, 2016.\nZihan Qiu, Zeyu Huang, Shuang Cheng, Yizhi Zhou, Zili Wang, Ivan Titov, and Jie Fu. Layerwise recurrent\nrouter for mixture-of-experts. arXiv preprint arXiv:2408.06793 , 2024.\nMichel Quak, Raquel Elea London, and Durk Talsma. A multisensory perspective of working memory.\nFrontiers in human neuroscience , 9:197, 2015.\nJack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive transformers\nfor long-range sequence modelling. arXiv preprint arXiv:1911.05507 , 2019.\nMarcus E Raichle, Ann Mary MacLeod, Abraham Z Snyder, William J Powers, Debra A Gusnard, and\nGordon L Shulman. A default mode of brain function. Proceedings of the national academy of sciences ,\n98(2):676\u2013682, 2001.\nJ Ranjith and Santhi Baskaran. Adaptive knowledge consolidation: A dynamic approach to mitigating\ncatastrophic forgetting in text-based neural networks. 2024.\nRyan V Raut, Abraham Z Snyder, and Marcus E Raichle. Hierarchical dynamics as a macroscopic organizing\nprincipleofthehumanbrain. Proceedings of the National Academy of Sciences , 117(34):20890\u201320897, 2020.\nDaniel Reznik, Robert Trampel, Nikolaus Weiskopf, Menno P Witter, and Christian F Doeller. Dissociating\ndistinct cortical networks associated with subregions of the human medial temporal lobe using precision\nneuroimaging. Neuron, 111(17):2756\u20132772, 2023.\nIvanRodkin, YuriKuratov, AydarBulatov, andMikhailBurtsev. Associativerecurrentmemorytransformer.\narXiv preprint arXiv:2407.04841 , 2024.\nEdmund T Rolls. The mechanisms for pattern completion and pattern separation in the hippocampus.\nFrontiers in systems neuroscience , 7:74, 2013.\nFr\u00e9d\u00e9ric Roux and Peter J Uhlhaas. Working memory and neural oscillations: alpha\u2013gamma versus theta\u2013\ngamma codes for distinct wm information? Trends in cognitive sciences , 18(1):16\u201325, 2014.\nJacob Russin, Randall C O\u2019Reilly, and Yoshua Bengio. Deep learning needs a prefrontal cortex. Work\nBridging AI Cogn Sci , 107(603-616):1, 2020.\nGabriel Sarch, Yue Wu, Michael J Tarr, and Katerina Fragkiadaki. Open-ended instructable embodied\nagents with memory-augmented large language models. arXiv preprint arXiv:2310.15127 , 2023.\n27\n",
  "project_dir": "artifacts/projects/enhanced_cs.CL_2508.10824v1_Memory_Augmented_Transformers_A_Systematic_Review",
  "communication_dir": "artifacts/projects/enhanced_cs.CL_2508.10824v1_Memory_Augmented_Transformers_A_Systematic_Review/.agent_comm",
  "assigned_at": "2025-08-17T20:59:05.108489",
  "status": "assigned"
}